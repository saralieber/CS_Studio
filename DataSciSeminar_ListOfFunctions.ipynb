{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataSciSeminar_ListOfFunctions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMe43nGgo/LrpbMoxW2+QUx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saralieber/CS_Studio/blob/master/DataSciSeminar_ListOfFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzoec6tbpmwQ",
        "colab_type": "text"
      },
      "source": [
        "# Review of Topics Covered in Data Science Seminar\n",
        "\n",
        "\n",
        "*   ### Ch.1 - Introduction to Manipulating Strings Using Methods, Lists, and Indexing\n",
        "*   ### Ch. 2 - Data Wrangling & For Loops\n",
        "*   ### Ch. 3 - K Nearest Neighbor \n",
        "    - A way of calculating similarty between cases using this formula: <img src='https://www.dropbox.com/s/9wao0kf3u32i3e9/euclidian.png?raw=1'>\n",
        "    - Examples: Calculated similarity between a chosen case (\"Braund\") and all other passengers based on which letters from the alphabet are contained in their names. Then, looked to see if the most similar passengers on this dimension scored similarly on the outcome variable (whether survived Tianic sinking or not)\n",
        "    - The outcome variable is also called the *label* column\n",
        "*   ### Ch. 4 - Introduction to Machine Learning\n",
        "    - Intro to ML algorithms -- in this course, we cover KNN (K Nearest Neighbor), Naive Bayes, and ANNs (Artificial Neural Nets) \n",
        "    - A *model* is an algorithm paired with training data. The model is used to make predictions on the testing data.\n",
        "\n",
        "    - Splitting data into *training* and *testing* sets\n",
        "    - Covers *KNN* algorithm applied to ML example\n",
        "    - Covers *Cosine Similarity* algorithm. Cosine similarity is a similarity measure that uses the equation: <img src='https://www.dropbox.com/s/oi1ttx99hf0uejn/cosine.png?raw=1'>\n",
        "\n",
        "*   ### Ch. 5 - spaCy & Naive Bayes\n",
        "    *   Basic Bayes Theorem\n",
        "\n",
        "<img src='https://www.dropbox.com/s/efpfgkenlit9rk1/Screenshot%202020-04-23%2009.42.25.png?raw=1' height=200>\n",
        "\n",
        "The jargony terms for the various pieces are as follows:\n",
        "\n",
        "   * P(A|B): The *posterior* probability.\n",
        "\n",
        "   * P(B|A): The *likelihood* or *conditional probability*.\n",
        "\n",
        "   * P(A): A *marginal* or *prior* probability.\n",
        "\n",
        "   * P(B): A *marginal* or *prior* probability.\n",
        "\n",
        "\n",
        "\n",
        "#### A More Complex Version of Bayes Theorem\n",
        "\n",
        "<img src='https://www.dropbox.com/s/gstzvvtvh9b39o8/bayes.png?raw=1'>\n",
        "\n",
        "   * O is equivalent to A from above; E is equivalent to B\n",
        "\n",
        "   * O stands for one of the \"classes\" being used. In our case, these are our three authors.\n",
        "\n",
        "   * E stands for the words in each sentence, in our case. More precisely, it's the spacy tokens that meet is_alpha and not is_stop.\n",
        "\n",
        "   * For instance, P(indefinite|EAP) means \"what is the probabilty of seeing the word indefinite in a sentence that EAP wrote?\"\n",
        "\n",
        "   * P(E) is the probabilty of seeing each word in the word bag.\n",
        "\n",
        "   * P(O) is the probability of a word being from one of the authors.\n",
        "\n",
        "\n",
        "*   Naive Bayes requires building a 'Word-Bag'\n",
        "\n",
        "\n",
        "### Ch. 6 - Other Considerations with Naive Bayes\n",
        "*   The Laplace Smoothing Factor\n",
        "    - A word may appear in the testing set that was not in the training set, and therefore would not be in our 'word-bag,' and would get assigned a probability of zero - not good!\n",
        "    - The Laplace smoothing factor adds 1 to each numerator (**it's like adding 1 to all the words in the word-bag**)\n",
        "    - This creates the problem that each author column will now be 1*|V|, where V is the vocabulary and |V| is the number of words we have in V (i.e., the length of the word-bag). To compensate, divide by |V|\n",
        "    - The result is you should never get a value of zero for any `P(word|author)\n",
        "*   Eliminating the denominator\n",
        "    - The denominator of the Naive Bayes equation can actually be ignored because it doesn't change the relationship between the probability of each outcome (i.e., the probability of which author used a particular word)\n",
        "*   Avoiding Underflow or The Vanishing Gradient\n",
        "   - Problem: taking the product of very small numbers gives a number close to zero, and there's a limit to how small a number Python can represent (turns it into a zero instead)\n",
        "   - Proposed Fix: If a product drops to zero (via underflow), we will set the product to the minimum numerical value that Python can represent - this avoids returning a zero probability from Naive Bayes\n",
        "\n",
        "### Ch. 7 (part 1) - Text as Vectors \n",
        "   - Defining Functions For Subtracting, Adding, and Dividing, Vectors\n",
        "   - Calculating the Mean of the Columns of a Matrix\n",
        "   - `ordered_embeddings` function (for calculating euclidean distance between a specific vector and other vectors in a table, and sort)\n",
        "   - `build_embedding_matrix` function (takes a string and a table as inputs and produces a matrix of values from it)\n",
        "\n",
        "### Ch. 7 (part 2) - More with spaCy\n",
        "*   Deriving meaning from text\n",
        "    - The distributional hypothesis (can derive words' meanings from their contexts)\n",
        "*   GloVe - Stanford's Global Vectors for Word Representation project\n",
        "    - A database of 300-Dimensional vectors regarding the contextual meaning for all the words in the `en_core_web_md` dictionary that comes with spaCy\n",
        "*   Calculating similarity between the meanings of different sentences based on their GloVe vectors\n",
        "\n",
        "### Ch. 8 - Artificial Neural Nets\n",
        "*   Another type of ML algorith. A model that has an input layer, a hidden layer, and an output layer.\n",
        "*   Takes an input vector, multiplies it by (like regression) weights (using the dot-product function), then passes these through an activation function (for non-linear solutions) to get the output\n",
        "*   Types of activation functions:\n",
        "    - RELU (rectified linear activation function) -- a piecewise linear function that will output the input directly if the output is positive; otherwise, it will output zero (if the input is negative) -- <img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png' height=200>\n",
        "    - Sigmoid -- produces a value between 0 and 1 -- <img src='https://www.dropbox.com/s/58hr9e4iusnmapc/Screenshot%202020-02-18%2014.02.06.png?raw=1'> <img src='https://www.dropbox.com/s/wdqdl22m2l7jruo/Screenshot%202020-02-18%2014.02.21.png?raw=1'>\n",
        "\n",
        "\n",
        "### Ch. 9 - ANNs with Numerical Data\n",
        "\n",
        "\n",
        "### Ch. 10 - CNNs (Convolutional Neural Nets)\n",
        "*   CNNs on 2D data -- like images!\n",
        "*   CNNs on 1D data -- like a sentence\n",
        "\n",
        "<img src='https://missinglink.ai/wp-content/uploads/2019/03/1D-convolutional-example_2x.png' height=300>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7_IddwoH_Ag",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBi3wyOlH_6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEJ5o5Ijliai",
        "colab_type": "text"
      },
      "source": [
        "# Import uo_puddles library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i2rIq2bfBj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flush the old uo_puddles directory and re-import\n",
        "!rm -r 'uo_puddles'\n",
        "my_github_name = 'uo-puddles' # can replace with your account name\n",
        "clone_url = f'https://github.com/{my_github_name}/uo_puddles.git'\n",
        "!git clone $clone_url\n",
        "import uo_puddles.uo_puddles as up"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP5dsdmYsylw",
        "colab_type": "text"
      },
      "source": [
        "# Store a Data Set in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3MLYKrFs0Og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "with open('content/gdrive/My Drive/titanic_letters.csv', 'w') as file:\n",
        "  table_name.to_csv(file, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5EK7qDYoNg2",
        "colab_type": "text"
      },
      "source": [
        "# Read the Data Back In"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f1gjlXUoRLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First Option to Read Data Back that's Been Stored in Google Drive\n",
        "with open('/content/gdrive/My Drive/word_bag_s20.csv', 'r') as file:\n",
        "    sorted_word_table = pd.read_csv(file, dtype={'word':str}, encoding='utf-8',\n",
        "                                    index_col='word', na_filter=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J95rMkWroWbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Second Option to Read Data Back that's Been Stored in Google Drive\n",
        "url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQR107nAfeU_z-p6sUv3yhnti9vNsklgXsm2RXAExQBHPUE3APm32qMQxTuYCEBbSz09MCVx-rnOXGb/pub?output=csv'\n",
        "sorted_word_bag = pd.read_csv(url, dtype={'word':str}, encoding='utf-8',\n",
        "                                index_col='word', na_filter=False)\n",
        "sorted_word_bag = sorted_word_bag.rename(index={'TRUE': 'true', 'FALSE': 'false'}) #need this because of bug in reading from url"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N-9ww29rBMV",
        "colab_type": "text"
      },
      "source": [
        "# String Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49NrjUMorDSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count Number of Time a Character or Phrase Occurs in a String\n",
        "FF.count('n') # count the number of times a specific character occurs in a string\n",
        "\n",
        "\n",
        "# Replace a Character or Phrase in a String with Another Character or Phrase\n",
        "FF.replace('diverged', 'coalesced') # replaces a character or word with a new character or word\n",
        "\n",
        "\n",
        "# Change Formatting of Characters in a String\n",
        "FF.capitalize() # capitalizes first character\n",
        "FF.casefold() # converts string to lower case\n",
        "FF.lower() # returns lower case version of string\n",
        "FF.upper() # converts string to upper case\n",
        "FF.swapcase() # changes lower case characters to upper case and vice versa\n",
        "FF.title() # converts first character of each word to upper case\n",
        "\n",
        "# Indexing Characters or Phrases in a String\n",
        "FF.index('v') # returns index of a character or phrase based on its FIRST instance in the string\n",
        "FF.rindex('v') # returns index of a character or phrase based on its LAST instance in the string\n",
        "FF.find('l') # returns the FIRST instance of a specific character by giving its index\n",
        "FF.rfind('l') # returns the LAST instance in the string where a character is found by giving its index\n",
        "\n",
        "\n",
        "# Checking Nature of the String\n",
        "FF.isalnum() # returns True if all characters in the string are alphanumeric\n",
        "FF.isalpha() # retruns True if all characters in the string are in the alphabet\n",
        "FF.isdecimal() # returns True if all characters in the string are decimals\n",
        "FF.isdigit() # returns True if all characters in the string are digits\n",
        "FF.isidentifier() # returns True if the string is an identifier (a string that contains only alphanumeric characters or underscores; cannot start with a number or contain any spaces)\n",
        "FF.islower() # returns True if all characters in the string are lower case\n",
        "FF.isnumeric() # returns True if all characters in the string are numeric\n",
        "FF.isprintable() # returns True if all characters in the string are printable\n",
        "FF.isspace() # returns True if all characters in the string are whitespaces\n",
        "FF.istitle() # returns True if the string follows the rules of a title (all words start with an upper case letter and the rest of the letters in the words are lower case; symbols and numbers are ignored)\n",
        "FF.isupper() # returns True if all characters in the string are upper case\n",
        "FF.endswith(\",\") # returns True/False for whether the string ends with the specified value\n",
        "FF.startswith(\"T\")\n",
        "\n",
        "\n",
        "# String Alignment\n",
        "FF.ljust(36) # returns left-aligned string; requires length of string argument\n",
        "FF.rjust(36) # returns right-aligned string; requires length of string argument\n",
        "FF.center(36) # center align a string; requires a length of string argument\n",
        "\n",
        "\n",
        "# Partition a String into Parts Based on a Given Keyword\n",
        "FF.partition('diverged') # returns a tuple based on the FIRST instance of the specified word in a string: 1) everything before the specific word, 2) the specified word, 3) everything after the specified word\n",
        "FF.rpartition('diverged') # same as partition but it bases the separation on the LAST instance of the specified word\n",
        "\n",
        "\n",
        "# Split a String into Parts Based on a Given Keyword\n",
        "FF.split('diverged') # Splits the string at the specified separator and returns a list based on FIRST instance of given word\n",
        "FF.rsplit('diverged') # Splits the string at the specified separator and returns a list based on LAST instance of given word\n",
        "\n",
        "\n",
        "# Strip Character(s) from Beginning or End of a String\n",
        "FF_for_lstrip = (',,,,,Two roads diverged in a yellow wood,')\n",
        "FF_for_lstrip.lstrip(',') # removes specified leading characters from the string\n",
        "FF_for_lstrip.rstrip(',') # trims from the right (notice no change in sentence occurred for the lstrip version of the string)\n",
        "\n",
        "\n",
        "# Joining Items in a Dictionary with a Given Separator\n",
        "Dict = {\"name\": \"John\", \"country\": \"Normway\"}\n",
        "Separator = \";\"\n",
        "Separator.join(Dict) # join items in a dictionary into a string using a given separator \n",
        "\n",
        "\n",
        "# Other Formatting Changes\n",
        "FF_for_format = 'Two {object} diverged in a {color} wood,' # the {word} are placeholders that can be modified with the format method\n",
        "FF_for_format.format(object = \"pandas\", color = \"pink\")\n",
        "  # Inside the placeholders, you can add formatting specifics, some examples:\n",
        "  ## :< left aligns the result\n",
        "  ## :e scientific format with a lowercase e\n",
        "  ## :n number format\n",
        "FF.encode() # encodes the string; default encoding is UTF-8\n",
        "FF.expandtabs() # Sets the tab size of the string\n",
        "\n",
        "\n",
        "# Fill the String with a Sepcified Number of 0 Values at the Beginning\n",
        "FF.zfill(50) # fill the string with zeros at the beginning until it is a given number of characters long"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_bnG6heraJE",
        "colab_type": "text"
      },
      "source": [
        "# Dropping Unwanted Columns from Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOaHuhLlriJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_list = ['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'] # Columns to drop\n",
        "trimmed_table = titanic_table.drop(drop_list, axis = 1) # the drop function drops the given columns from the dataset; axis=1 means drop columns (axis=0 drops rows, which is default)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsgV_6porwXN",
        "colab_type": "text"
      },
      "source": [
        "# Convert a Column Variable to a list\n",
        "### (Need to do this to work with raw text data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvFiNUVCr04e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_names = trimmed_table['Name'].tolist() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHEqxw0Tr8cY",
        "colab_type": "text"
      },
      "source": [
        "# Create a New List from an Old List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVX6gFDrr-IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_list = []\n",
        "\n",
        "for i in range(len(list_of_names)):\n",
        "  items_old_list = old_list[i]\n",
        "  new_list_items = len(items_old_list)\n",
        "  new_list.append(new_list_items)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7E67j7LsPFR",
        "colab_type": "text"
      },
      "source": [
        "# Add a Column to a Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XvXGvl8sQly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trimmed_table['Length'] = lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puS_aPMHI380",
        "colab_type": "text"
      },
      "source": [
        "# Index a Row from a Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCt-RtBpI697",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table_name.iloc[0].tolist() # and also convert the output to a list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxU0tUGxsUk_",
        "colab_type": "text"
      },
      "source": [
        "# Calculate number of times each letter from the alphabet occurs in names of passengers from the Titanic data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2dOFw7GsYH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "for i in range(len(alphabet)):\n",
        "  alpha = alphabet[i] # take each letter of the alphabet at a time\n",
        "  alpha_counts = []   # create an empty list 27 times\n",
        "\n",
        "  for j in range(len(list_of_names)):\n",
        "    name = list_of_names[j] # obtain each passenger name \n",
        "    lower_name = name.lower() # convert each passenger name to lower case to match the case of the alphabet list\n",
        "    alpha_count = lower_name.count(alpha) # count the number of times each letter of the alphabet occurs in each name, one at a time\n",
        "    alpha_counts.append(alpha_count) # add the count for each letter to the empty list for each name and each letter of the alphabet\n",
        "  trimmed_table[alpha] = alpha_counts # create 27 new columns for each letter of the alphabet populated with the counts for each name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB_KUW6kHfvo",
        "colab_type": "text"
      },
      "source": [
        "# Calculate Average Number of Words per Sentence in a Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYRPJrAvHiH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mystery_poem = '''\n",
        "Dust always blowing about the town,\n",
        "Except when sea-fog laid it down,\n",
        "And I was one of the children told\n",
        "Some of the blowing dust was gold.\n",
        "\n",
        "All the dust the wind blew high\n",
        "Appeared like god in the sunset sky,\n",
        "But I was one of the children told\n",
        "Some of the dust was really gold.\n",
        "\n",
        "Such was life in the Golden Gate:\n",
        "Gold dusted all we drank and ate,\n",
        "And I was one of the children told,\n",
        "\"We all must eat our peck of gold\"\n",
        "'''\n",
        "\n",
        "# First, remove punctuation or other characters that are not part of words\n",
        "mystery_poem = mystery_poem.replace(',','')\n",
        "mystery_poem = mystery_poem.replace(':','')\n",
        "mystery_poem = mystery_poem.replace('\"','')\n",
        "\n",
        "# Split by sentence (using a period to indicate end of each sentence)\n",
        "poem_sentences = mystery_poem.split('.')\n",
        "poem_sentences\n",
        "\n",
        "n = len(poem_sentences)\n",
        "\n",
        "word_counts = []\n",
        "\n",
        "for i in range(n):\n",
        "  sentence = poem_sentences[i]\n",
        "  words = sentence.split()\n",
        "  counts = len(words)\n",
        "  word_counts.append(counts)\n",
        "print(word_counts)\n",
        "\n",
        "average = sum(word_counts)/3\n",
        "average"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5aMzSepMAQe",
        "colab_type": "text"
      },
      "source": [
        "# Create a Matrix out of Rows with Specified Columns from Original Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxr3ZPELMIx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(letters_table) # length of original table (number of rows)\n",
        "\n",
        "all_passengers = [] # blank new matrix\n",
        "\n",
        "for i in range(n):\n",
        "  rows = letters_table.iloc[i].tolist() # convert each row to a list of numbers\n",
        "  reducing_rows = rows[2:] # only want to keep the third column and on\n",
        "  all_passengers.append(reducing_rows) # append chosen rows to the new matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yehtt72XJgmS",
        "colab_type": "text"
      },
      "source": [
        "# Calculate Euclidean Distance Between Two Chosen Cases using `euclidean_distance` function from the UO puddles (up) library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf91t-zQJ6tP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Either import the uo_puddles library or include the function definition:\n",
        "\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def euclidean_distance(vect1:list ,vect2:list) -> float:\n",
        "  assert isinstance(vect1, list), f'vect1 is not a list but a {type(vect1)}'\n",
        "  assert isinstance(vect2, list), f'vect2 is not a list but a {type(vect2)}'\n",
        "  assert len(vect1) == len(vect2), f\"Mismatching length for euclidean vectors: {len(vect1)} and {len(vect2)}\"\n",
        "  '''\n",
        "  sum = 0\n",
        "  for i in range(len(vect1)):\n",
        "      sum += (vect1[i] - vect2[i])**2\n",
        "      \n",
        "  #could put assert here on result   \n",
        "  return sum**.5  # I claim that this square root is not needed in K-means - see why?\n",
        "  '''\n",
        "  a = np.array(vect1)\n",
        "  b = np.array(vect2)\n",
        "  return norm(a-b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfInYhS9JoOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "up.euclidean_distance(braund_number_list, allen_number_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7qS-vJpKVhu",
        "colab_type": "text"
      },
      "source": [
        "# Create a Matrix of All the Euclidean Distances Between a Specific Case and the Other Rows in a Table and Sort by Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMax3fXLKcYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_distances_matrix = []\n",
        "\n",
        "for i in range(n):\n",
        "  passenger_list = all_passengers[i]\n",
        "  braund = braund_number_list\n",
        "  distances = up.euclidean_distance(braund, all_passengers[i])\n",
        "  all_distances_matrix.append([distances, i]) # append new matrix with euclidean distances and index numbers\n",
        "\n",
        "sorted_distances_matrix = sorted(all_distances_matrix, key = lambda x:x[1])  #sorted will default to using the first item in our 2-item lists\n",
        "\n",
        "\n",
        "# Look at top 10 most similar cases to the chosen comparison case in sorted table\n",
        "sorted_distances_matrix[:10]\n",
        "\n",
        "# Pick out the row for the case most similar (at the top of the sorted table)\n",
        "letters_table.iloc[477].tolist()  #go back to original table and see\n",
        "\n",
        "\n",
        "# Write a for loop to get the top four most similar cases from the sorted table\n",
        "top4 = [477, 482, 804, 806] # Index numbers of top four most similar cases from the sorted table\n",
        "\n",
        "for i in range(4):\n",
        "  index = top4[i]  #477, then 482, then 804, then 806\n",
        "  row = letters_table.iloc[index].tolist() # identify the row from the table of each index and convert to a list\n",
        "  outcome = row[0]  #The first value in row from the letters_table is the Survived value, 0 (survived) or 1 (didn't survive)\n",
        "  print(outcome) # Print the survived scores for the four people grabbed and see if they have the same outcome as the comparison case"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFfjE4wARxHS",
        "colab_type": "text"
      },
      "source": [
        "# The `ordered_distances_matrix` function from the UO puddles library does the same thing done above\n",
        "   - Meaning it calculates the Euclidean distance between a reference case and all other cases, then sorts the cases from smallest distance to largest distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGODfE8oSH8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ordered_distances_matrix(target_vector:list, crowd_matrix:list,  dfunc=euclidean_distance) -> list:\n",
        "  assert isinstance(target_vector, list), f'target_vector not a list but instead a {type(target_vector)}'\n",
        "  assert isinstance(crowd_matrix, list), f'crowd_matrix not a list but instead a {type(crowd_matrix)}'\n",
        "  assert all([isinstance(row, list) for row in crowd_matrix]), f'crowd_matrix not a list of lists'\n",
        "  assert all([len(target_vector)==len(row) for row in crowd_matrix]), f'crowd_matrix has varied row lengths'\n",
        "  assert callable(dfunc), f'dfunc not a function but instead a {type(dfunc)}'\n",
        "\n",
        "  distance_list = [(index, dfunc(target_vector, row)) for index, row in enumerate(crowd_matrix)]\n",
        "  return sorted(distance_list, key=lambda pair: pair[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2lfvcg2ni9M",
        "colab_type": "text"
      },
      "source": [
        "# Import spacy (text analysis library) and a commonly used dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqCH8DzPnllj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_md # download the dictionary\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdV_qcqnYYkW",
        "colab_type": "text"
      },
      "source": [
        "# Parse a string into words (spaCy 'tokens') using `doc` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTw0ubL1Ylui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(practice_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10IOnCaoYqlZ",
        "colab_type": "text"
      },
      "source": [
        "# Convert those spaCy 'tokens' back into strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0MVt7F-Y49F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text)  #using the text attribute - gives me a string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kbuk2CyZGMe",
        "colab_type": "text"
      },
      "source": [
        "# Display all text when displaying a table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUBz1cMeZJTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)  #None forces all of sentence to be shown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQ6mOHjVFab",
        "colab_type": "text"
      },
      "source": [
        "# Check if a Word is Contained in spaCy's Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lEhphSbVLFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp.vocab.has_vector('frankenstein') # Check to make sure word vectors have been loaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBXZ88pinzke",
        "colab_type": "text"
      },
      "source": [
        "# `get_vec` function\n",
        "\n",
        "Stanford has a project called Global Vectors for Word Representation (GloVe), which contains 300-dimensional vectors associated with all of the words in spaCy's vocabulary. \n",
        "\n",
        "The vector for each word represents its semantic value according to the distributional hypothesis from linguistics - that you can derive the meaning of words based on their contexts.\n",
        "\n",
        "The get_vec function gets the 300-dimensional vector associated with a word in spaCy's vocabulary and converts the vector to a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1FzOGIOn3za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vec(s:str) -> list:\n",
        "  return nlp.vocab[s].vector.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y_kuwhpVuV3",
        "colab_type": "text"
      },
      "source": [
        "# Compare Similarity of Two Text Items that Are Associated with Numerical Values using Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sEg1x1QVyoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "up.cosine_similarity(get_vec('dog'), get_vec('puppy')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvVAC6nollsl",
        "colab_type": "text"
      },
      "source": [
        "# `ordered_embeddings` function\n",
        "\n",
        "This function takes a vector and a table and calculates the euclidean distance between the item represented by the vector and the items in each row of the table.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LegE01qllfW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ordered_embeddings(target_vector, table): # define a new function, called ordered_embeddings, that takes a target_vector and a table as inputs\n",
        "                                              # the target_vector needs to be a list (convert to a list if it's not)\n",
        "  names = table.index.tolist() # names is a list of the indexes from the provided table (in this case, would be the animal names)\n",
        "  ordered_list = [] # the results ordering difference between each animal and the target_animal will be listed in order here\n",
        "  for i in range(len(names)): # for each animal row\n",
        "    name = names[i] # name is an interation of each row in the animal table\n",
        "    row = table.loc[name].tolist() # convert each row to a list\n",
        "    d = up.euclidean_distance(target_vector, row) # calculate distance between the target_animal and the animal in each other row of the table\n",
        "    ordered_list.append([d, names[i]]) # fill the ordered_list with the calculated distances and names of each animal\n",
        "  ordered_list = sorted(ordered_list) # sort the list from lowest to highest distance\n",
        "\n",
        "  return ordered_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XXnfQ8Nl2l-",
        "colab_type": "text"
      },
      "source": [
        "# `subtractv` function\n",
        "\n",
        "Subectract two vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnB1hiKul7US",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subtractv(x:list, y:list) -> list:\n",
        "  assert isinstance(x, list), f\"x must be a list but instead is {type(x)}\"\n",
        "  assert isinstance(y, list), f\"y must be a list but instead if {type(y)}\"\n",
        "  assert len(x) == len(y), f\"x and y must be the same length\"\n",
        "\n",
        "  result = [] # blank list to contain results of subtracting each item in x and y\n",
        "  for i in range(len(x)):\n",
        "    c1 = x[i]\n",
        "    c2 = y[i]\n",
        "    result.append(c1-c2)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K76Sj9fNl_94",
        "colab_type": "text"
      },
      "source": [
        "# `addv` function\n",
        "\n",
        "Add two vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02efxER1mCE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addv(x:list, y:list) -> list: # define a function, called addv, that takes variables x & y (both lists) as inputs\n",
        "  assert isinstance(x, list), f\"x must be a list but instead is {type(x)}\"\n",
        "  assert isinstance(y, list), f\"y must be a list but instead is {type(y)}\"\n",
        "  assert len(x) == len(y), f\"x and y must be the same length\"\n",
        "\n",
        "  #result = [(c1 - c2) for c1, c2 in zip(x, y)]  #one-line compact version - called a list comprehension\n",
        "\n",
        "  result = []\n",
        "  for i in range(len(x)):\n",
        "    c1 = x[i]\n",
        "    c2 = y[i]\n",
        "    result.append(c1+c2)\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TneWN27DmEvU",
        "colab_type": "text"
      },
      "source": [
        "# `dividev` function\n",
        "\n",
        "Divide a vector by a constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovwtqS_imIqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dividev(x:list, y:int) -> list:\n",
        "  assert isinstance(x, list), f\"x must be a list but instead is {type(x)}\"\n",
        "  assert isinstance(y, int), f\"y must be an integer but instead is {type(y)}\"\n",
        "\n",
        "  result = []\n",
        "  for i in range(len(x)): \n",
        "    c1 = x[i]\n",
        "    result.append(c1/y)\n",
        "  return result "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCuRw9ZLmK7O",
        "colab_type": "text"
      },
      "source": [
        "# `meanv` function\n",
        "\n",
        "Calculate the mean vector from a matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBX5ptHMmOfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def meanv(matrix:list) -> list:\n",
        "  assert isinstance(matrix, list), f\"matrix must be a list but instead is {type(x)}\"\n",
        "  assert len(matrix) >=1, f\"matrix must have at least one row\"\n",
        "\n",
        "  sumv = matrix[0] # start with the first row\n",
        "  for row in matrix[1:]: # add each row to the first row, starting with the second row\n",
        "    sumv = addv(sumv, row) # take the sum of the first+second row, and then this resulting sum plus the third row\n",
        "  mean = dividev(sumv, len(matrix)) # divide the sum of all the rows by the number of rows\n",
        "  return mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4AJrpLImTKG",
        "colab_type": "code",
        "outputId": "10c2b71f-a306-43aa-8340-d029ce29ee6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Test the meanv function using matrix A\n",
        "\n",
        "A = [[0,1], \n",
        "     [2,2], \n",
        "     [4,3]]\n",
        "\n",
        "A[0] # [0,1]\n",
        "A[1] # [2,2]\n",
        "A[2] # [4,3]\n",
        "len(A) # 3\n",
        "\n",
        "# Expected result - note: the numbers in corresponding positions get added using the addv function\n",
        "## First iteration:\n",
        "# sumv = A[0] + A[1] = ([0,1]+[2,2]) = [2,3]\n",
        "## Second iteration:\n",
        "# sumv = [2,3] + A[2] = ([2,3]+[4,3]) = [6,6]\n",
        "## mean = [6,6]/3 = [2.0,2.0]\n",
        "\n",
        "meanv(A) # Finds the mean of each column"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.0, 2.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60EX7EC8a1VK",
        "colab_type": "text"
      },
      "source": [
        "# `dot` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoE1y8lva21f",
        "colab_type": "text"
      },
      "source": [
        "Compute the dot-product of two vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNjn0BKSa9M0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dot(vector1: list, vector2:list) -> float:\n",
        "  assert isinstance(vector1, list), f'vector1 should be a list but is instead a {type(vector1)}'\n",
        "  assert isinstance(vector2, list), f'vector2 should be a list but is instead a {type(vector2)}'\n",
        "  assert len(vector1) == len(vector2), f'both vectors should be the same length'\n",
        "\n",
        "  result = 0\n",
        "  for i in range(len(vector1)):\n",
        "    term = vector1[i]*vector2[i]\n",
        "    result += term\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UaCUuaQWCYi",
        "colab_type": "text"
      },
      "source": [
        "# Sentence Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m89WfFGJWD84",
        "colab_type": "text"
      },
      "source": [
        "## Converting an entire sentence into a single GloVe vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J12HT3pUXYaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pilot_sentences = [\n",
        "  'It was really cold yesterday.',\n",
        "  'It will be really warm today, though.',\n",
        "  \"It'll be really hot tomorrow!'\",\n",
        "  'Will it be really cool Tuesday?'\n",
        "]\n",
        "\n",
        "# For a single sentence\n",
        "first_sent = pilot_sentences[0]\n",
        "doc = nlp(first_sent.lower())\n",
        "\n",
        "vectors = []\n",
        "\n",
        "for token in doc:\n",
        "  word = token.text\n",
        "  vecs = get_vec(word)\n",
        "  vectors.append(vecs)\n",
        "\n",
        "s0_average = meanv(vectors)\n",
        "len(s0_average) # 300\n",
        "print(s0_average[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3nVUQanpW5Z",
        "colab_type": "text"
      },
      "source": [
        "# `sent2vec` function\n",
        "\n",
        "Takes a sentence (a raw string) as an argument and produces the average GloVe vector for it.\n",
        "\n",
        "If you run across a sentence that adds nothing to the matrix (because it has no legal tokens), return this value: [0.0]*300 - which builds a 300-dimensional matrix of all zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWnqN6ImHbt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent2vec(raw_text: str):\n",
        "  assert isinstance(raw_text, str), f'raw_text should be string but instead is {type(raw_text)}'\n",
        "\n",
        "  matrix = [] \n",
        "  doc = nlp(raw_text.lower()) \n",
        "\n",
        "  for token in doc: \n",
        "    if token.is_alpha and not token.is_stop:\n",
        "      word = token.text # convert each word to a string\n",
        "      vectors = get_vec(word) # get the 300-D GloVe vector for each word\n",
        "      matrix.append(vectors) # add each of these 300-D vectors to the blank matrix\n",
        "      if token.is_stop or not token.is_alpha:\n",
        "          return ([0.0]*300) # this line needs to be updated\n",
        "  return meanv(matrix) # return the mean of the matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnVEgrKTYOq-",
        "colab_type": "text"
      },
      "source": [
        "# Finding Similarity among Sentences\n",
        "\n",
        "*   First, GloVe-ify all the sentences and put in a matrix (in this case, the sentences from Dracula)\n",
        "*   Then, choose a comparison sentence and use sent2vec to get its GloVe vector\n",
        "*   Use cosine similarity to compare GloVe vectors of the comparison sentence and all sentences from chosen text \n",
        "*   Sort the final result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgj536thYV1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drac_matrix = []\n",
        "\n",
        "for i in range(len(drac_sentences)):  #we defined drac_sentences above\n",
        "  sentence = drac_sentences[i]\n",
        "  vec = sent2vec(sentence.text)\n",
        "  drac_matrix.append(vec)\n",
        "\n",
        "# Test sentence\n",
        "test_sentence = \"My favorite food is strawberry ice cream.\"\n",
        "\n",
        "\n",
        "# Find sentences in Dracula closest to the test sentence using sent2vec\n",
        "# Based on cosine similarity\n",
        "input_vec = sent2vec(test_sentence)\n",
        "\n",
        "ordered_distances = []\n",
        "\n",
        "for i in range(len(drac_matrix)):  #we defined drac_sentences above\n",
        "  vec = drac_matrix[i]\n",
        "  d = up.fast_cosine(np.array(input_vec), np.array(vec))  #using speedier version that relies on numpy\n",
        "  ordered_distances.append([d, i])\n",
        "\n",
        "for d,j in sorted(ordered_distances, reverse=True)[:10]:\n",
        "  print(drac_sentences[j])\n",
        "  print('=========') # puts this after each sentence so it's easy to tell them apart"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Nug1Tjmllf",
        "colab_type": "text"
      },
      "source": [
        "# `build_embedding_matrix` function\n",
        "\n",
        "Takes a string (e.g., a book) and a table (e.g., a table with all the colors represented by their RGB codes) as inputs and produces a matrix of values from it (e.g., the RGB values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHhY5xHcmvgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_embedding_matrix(raw_text: str, table) -> list:\n",
        "  assert isinstance(raw_text, str), f'raw_text should be string but instead is {type(raw_text)}'\n",
        "  assert isinstance(table, pd.core.frame.DataFrame), f'table not a dataframe but instead a {type(table)}'\n",
        "  assert 'nlp' in globals(), f'This function assumes that the spacy nlp function has been defined'\n",
        "\n",
        "  matrix = [] # a blank matrix that will be filled\n",
        "  index_list = table.index.tolist() # convert the index of the table (in this example, color names) to a list\n",
        "  doc = nlp(raw_text.lower()) # parse the raw text (in this case, the text from a book)\n",
        "\n",
        "  # short version\n",
        "  # matrix = [table.loc[token.text].tolist() for token in doc if token.text in index_list]\n",
        "\n",
        "  for token in doc: # for each word in the parsed book\n",
        "    word = token.text # convert each word to a string\n",
        "    if word in index_list: # if any word from the book is also in the list of color names\n",
        "      matrix.append(table.loc[word].tolist()) # add that word to the blank matrix\n",
        "  return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-I9dPKDVlmt",
        "colab_type": "text"
      },
      "source": [
        "# `zip` function\n",
        "\n",
        "Pairs together values in corresponding column positions from two separate lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78UBdzoyVn_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_A = [1,2,3]\n",
        "list_B = [4,5,6]\n",
        "\n",
        "zipped = list(zip(list_A, list_B))\n",
        "zipped # [(1, 4), (2, 5), (3, 6)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i39HaTApilq",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning with KNN and/or Cosine Similarity\n",
        "\n",
        "1. Randomly shuffle the rows of the original data set.\n",
        "2. Decide the percentage of data you want in the training/testing set. Calculate the sample size needed for each.\n",
        "3. Split data into training & testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iEFrcNcQOuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle the rows of the full data set randomly\n",
        "set_seed = 1234 # to be able to replicate random shuffling\n",
        "import numpy as np\n",
        "rsgen = np.random.RandomState(set_seed) # numpy's random number generator\n",
        "\n",
        "# Use the .sample() method to randomly shuffle the rows of the original table\n",
        "shuffled_table = letters_table.sample(frac=1, random_state = rsgen) # frac=1 means shuffle the entire table\n",
        "shuffled_table.head()\n",
        "\n",
        "\n",
        "# Calculate number of rows for the testing set by dividing the total length of the table by 3 (~33%)\n",
        "# For this example, we want 30% of the full data set to go into the testing set (aka holdout sample), and 70% to go into the training set\n",
        "## Because prefer more data to train the model on\n",
        "n_for_test_set = int(len(letters_table)/3) # using int() rounds the result to the nearest whole number\n",
        "n_for_test_set \n",
        "\n",
        "# Do the same for calculating number of rows for the training set by subtracting total length of the data by number of rows going into testing set\n",
        "n_for_train_set = len(letters_table) - n_for_test_set # the other 70% goes in the training set\n",
        "n_for_train_set \n",
        "\n",
        "\n",
        "\n",
        "## Split the randomly shuffled table into training & testing sets based on n's above\n",
        "testing_table = shuffled_table[:n_for_test_set]\n",
        "testing_table = testing_table.reset_index(drop=True) # reset the indices \n",
        "\n",
        "training_table = shuffled_table[n_for_test_set:]\n",
        "training_table = training_table.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Convert the training table into a matrix of lists taking only the third column and on\n",
        "training_passengers_matrix = []\n",
        "\n",
        "for i in range(len(training_table)):\n",
        "  rows = training_table.iloc[i].tolist()\n",
        "  sliced_rows = rows[2:]\n",
        "  training_passengers_matrix.append(sliced_rows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx9xshebTIMf",
        "colab_type": "text"
      },
      "source": [
        "# Store the Scores on the Outcome Variable for the Training Set \n",
        "   - Note: We're doing supervised learning for building the ML model since the training set has known scores on the outcome variable (called labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9neeMrq8TRhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_labels = letters_table['Survived'].tolist() # 'Survived' is the outcome variable (0 = died, 1 = survived)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0brvJBpZTW5b",
        "colab_type": "text"
      },
      "source": [
        "# Apply the KNN Algorithm to build a model and make predictions about cases in the testing set\n",
        "   - Using the knn function from the UO puddles library\n",
        "   - This produces a prediction for our reference person from the testing set based on who they are most similar to from the training set & what the majority outcome of those people was"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJNQMEGNTdIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# up.knn(testing_set_cases, training_set_cases, training_cases_outcomes, value_of_k, similarity_measure)\n",
        "# k is a hyperparameter specified by the researcher (in this case, 5)\n",
        "\n",
        "n = len(testing_table)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(n):\n",
        "  rows = testing_table.iloc[i].tolist()\n",
        "  values = rows[2:]\n",
        "  preds = up.knn(values, training_passengers_matrix, training_labels, 5, 'euclidean')\n",
        "  predictions.append(preds)\n",
        "print(predictions)\n",
        "\n",
        "# Were our predictions correct?\n",
        "\n",
        "testing_labels = testing_table['Survived'].tolist() # Store actual scores on the outcome variable to a list\n",
        "cases = list(zip(predictions,testing_labels)) # Pair together predictions with actual scores on outcome variable\n",
        "print(cases[:10])  #[(1, 1), (1, 0), (0, 0), (1, 1), (0, 0), (1, 0), (0, 0), (1, 0), (0, 1), (0, 0)] # Evaluate which predictions were correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9H6tW0WUwTU",
        "colab_type": "text"
      },
      "source": [
        "# The code for the knn function from UO puddles library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3E7pSP1TuQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def knn(target_vector:list, crowd_matrix:list,  labels:list, k:int, sim_type='euclidean') -> int:\n",
        "  assert isinstance(target_vector, list), f'target_vector not a list but instead a {type(target_vector)}'\n",
        "  assert isinstance(crowd_matrix, list), f'crowd_matrix not a list but instead a {type(crowd_matrix)}'\n",
        "\n",
        "  #assert sim_type in sim_funs, f'sim_type must be one of {list(sim_funs.keys())}.'\n",
        "    \n",
        "  if sim_type in ['pearson', 'linear', 'correlation']:\n",
        "    distance_list = [[index, abs(np.corrcoef(np.array(target_vector), np.array(row))[0][1])] for index,row in enumerate(crowd_matrix)]\n",
        "    direction = True\n",
        "  else:\n",
        "    sim_funs = {'euclidean': [euclidean_distance, False], 'cosine': [cosine_similarity, True]}\n",
        "    dfunc = sim_funs[sim_type][0]\n",
        "    distance_list = [[index, dfunc(target_vector, row)] for index,row in enumerate(crowd_matrix)]\n",
        "    direction = sim_funs[sim_type][1]\n",
        "\n",
        "  sorted_crowd =  sorted(distance_list, key=lambda pair: pair[1], reverse=direction)  #False is ascending\n",
        "\n",
        "  #Compute top_k\n",
        "  top_k = [i for i,d in sorted_crowd[:k]]\n",
        "  #Compute opinions\n",
        "  opinions = [labels[index] for index in top_k]\n",
        "  #Compute winner\n",
        "  winner = 1 if opinions.count(1) > opinions.count(0) else 0\n",
        "  #Return winner\n",
        "  return winner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIQt995iVMVj",
        "colab_type": "text"
      },
      "source": [
        "# Apply the Cosine Similarity Algorithm to build a model and make predictions about cases in the testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZnD8gN2VcUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(testing_table)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(n):\n",
        "  rows = testing_table.iloc[i].tolist()\n",
        "  values = rows[2:]\n",
        "  preds = up.knn(values, training_passengers_matrix, training_labels, 5, 'cosine')\n",
        "  predictions.append(preds)\n",
        "print(predictions)\n",
        "\n",
        "# Were our predictions correct?\n",
        "\n",
        "testing_labels = testing_table['Survived'].tolist() # Store actual scores on the outcome variable to a list\n",
        "cases = list(zip(predictions,testing_labels)) # Pair together predictions with actual scores on outcome variable\n",
        "print(cases[:10])  #[(1, 1), (1, 0), (0, 0), (1, 1), (0, 0), (1, 0), (0, 0), (1, 0), (0, 1), (0, 0)] # Evaluate which predictions were correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqcJVCy1WGQE",
        "colab_type": "text"
      },
      "source": [
        "# Accuracy of ML Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUw6noALWKXc",
        "colab_type": "text"
      },
      "source": [
        "## Idea of True Positive, False Positive, True Negatives, and False Negatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PszNp73WJDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('True positive: ', cases.count((1,1)))  #True positive:  69\n",
        "print('True negative: ', cases.count((0,0)))  #True negative:  116\n",
        "print('False positive: ', cases.count((1,0))) #False positive:  68\n",
        "print('False negative: ', cases.count((0,1))) #False negative:  44"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xQTL8wLWSin",
        "colab_type": "text"
      },
      "source": [
        "## One way of assessing accuracy is the number of true positive and negatives out of total number of predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNMFppJ1WkAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(69+116)/testing_n  #0.622895622895623"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCVypyXhWpll",
        "colab_type": "text"
      },
      "source": [
        "# Other ways of defining accuracy\n",
        "\n",
        "<img src='https://www.dropbox.com/s/zubecbzi8zsdzgg/confusion_matrix.png?raw=1'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X536StyYqk3z",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning with Naive Bayes Algorithm\n",
        "\n",
        "1. Randomly shuffle the rows of the original data set. Split into a training set (70%) and testing set (30%).\n",
        "2. Build a \"bag of words\" from the training table to use with the Naive Bayes Algorithm.\n",
        "3. Apply Naive Bayes to build a model.\n",
        "4. Evaluate on testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVDMPCYZaDGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1. Randomly shuffle the rows of gothic_sentences. Split into a training set (70%) and testing set (30%).\n",
        "\n",
        "set_seed = 1234\n",
        "\n",
        "import numpy as np\n",
        "rsgen = np.random.RandomState(set_seed)\n",
        "\n",
        "\n",
        "# Shuffled Gothic Sentences\n",
        "shuffled_gothics = gothic_sentences.sample(frac=1, random_state = rsgen).reset_index(drop=True)\n",
        "len(shuffled_gothics)\n",
        "\n",
        "\n",
        "# Calculating n's for Testing and Training Tables\n",
        "n_testing = (len(shuffled_gothics))*.3\n",
        "n_testing # 5874\n",
        "\n",
        "n_training = (len(shuffled_gothics)) - n_testing\n",
        "n_training # 13705\n",
        "\n",
        "\n",
        "# Training Set\n",
        "training_table = shuffled_gothics[:13705].reset_index(drop=True)\n",
        "\n",
        "# Testing Set\n",
        "testing_table = shuffled_gothics[13705:].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Grab the Text and Author Columns from the Training and Testing Sets and Convert into Lists (easier to work with text data this way)\n",
        "training_text = training_table['text'].tolist()\n",
        "training_authors = training_table['author'].tolist()\n",
        "\n",
        "testing_text = testing_table['text'].tolist()\n",
        "testing_authors = testing_table['author'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKe3wdHoqyZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2. Build a \"bag of words\" from the training table.\n",
        "\n",
        "# Create an empty dataframe that will be the \"word bag\"\n",
        "word_bag = pd.DataFrame(columns=['word','EAP','MWS','HPL']) # build a dataframe with columns for each word in the gothic texts and each authors' name abbreviated\n",
        "word_bag.head() # currently empty\n",
        "\n",
        "\n",
        "# See how you can add a row\n",
        "row0 = ['indefinite',1,0,0]\n",
        "word_bag.loc[0] = row0\n",
        "word_bag.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84y6yzkOacew",
        "colab_type": "text"
      },
      "source": [
        "## There's an `update_gothic_row` function in UO puddles library that updates rows in an empty word-bag \n",
        "*   The update_gothic_row function takes a word and an author. \n",
        "*   It first checks to see if the word is already in the table. If it is not, it creates a row for it.\n",
        "*   It then finds the column that goes with the author and increments the value by 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb1YZHAdbe1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A function in uo_puddles has been written to add rows the way we did above (up.update_gothic_row)\n",
        "word_table = pd.DataFrame(columns=['word', 'EAP', 'MWS', 'HPL'])\n",
        "up.update_gothic_row(word_bag, 'indefinite', 'EAP') # acts as a counter - adds another count underneath the author given for the corresponding word; if the word doesn't exist in the dataframe yet, it adds a new row for that word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qglAXFUumMWW",
        "colab_type": "text"
      },
      "source": [
        "# `update_gothic_row` function from UO puddles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zQN8p7OmLvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_gothic_row(word_table, word:str, author:str):\n",
        "  assert author in word_table.columns.tolist(), f'{author} not found in {word_table.columns.tolist()}'\n",
        "\n",
        "  word_list = word_table['word'].tolist()\n",
        "  real_word = word if type(word) == str else word.text\n",
        "\n",
        "  if real_word in word_list:\n",
        "    j = word_list.index(real_word)\n",
        "  else:\n",
        "    j = len(word_table)\n",
        "    word_table.loc[j] = [real_word] + [0]*(len(word_table.columns)-1)\n",
        "\n",
        "  word_table.loc[j, author] += 1\n",
        "\n",
        "  return word_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJYVzH58ma7R",
        "colab_type": "text"
      },
      "source": [
        "# Create a 'Word-Bag' Using a For Loop that Goes Through All Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrOkgovvmqe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reminder of what training_text and training_authors are:\n",
        "## training_text = training_table['text'].to_list()\n",
        "## The 'Text' column from the training set (which was assembled from taking 70% of the rows from a shuffled table with the original data)\n",
        "\n",
        "# training_authors = training_table['author'].to_list()\n",
        "## The 'Authors' column from the training set\n",
        "\n",
        "word_table = pd.DataFrame(columns=['word', 'EAP', 'MWS', 'HPL'])  # Empty word-bag\n",
        "\n",
        "for i in range (len(training_text)): # We're building the word-bag from the training set\n",
        "  training_sentences = training_text[i].lower() # Each lowercase word in the training_text\n",
        "  doc = nlp(training_sentences) # Tokenize each word in the training text\n",
        "  author = training_authors[i] # Take each author name from the training_authors list\n",
        "\n",
        "  for token in doc: # For each token in doc\n",
        "    if token.is_alpha and not token.is_stop # Remove cases that are not a letter and only keep cases that are not stop words\n",
        "      up.update_gothic_row(word_table, token.text, author) # Apply update_gothic_row function to each word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuEct83EnrBB",
        "colab_type": "text"
      },
      "source": [
        "# Sort the Word Bag Alphabetically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI3O95cTnsvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted_word_table = word_table.sort_values(by=['word'])\n",
        "sorted_word_table = sorted_word_table.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs7PqIHOnwBQ",
        "colab_type": "text"
      },
      "source": [
        "# Set a New Index for a Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5NtLHB5nxbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted_word_table = sorted_word_table.set_index('word')  #set the word column to be the table index\n",
        "sorted_word_table.head() \n",
        "\n",
        "# Now, you can pull a row based on the word you want\n",
        "sorted_word_table.loc['indefinite'].tolist()  # **Use loc instead of iloc**\n",
        "\n",
        "# Can also use loc to get a specific column in a row (can't do this with iloc)\n",
        "sorted_word_table.loc['indefinite', 'EAP']  # give index first then column name to get a cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfMWi0ptw2v8",
        "colab_type": "text"
      },
      "source": [
        "# Build a ML Model using Naive Bayes algorithm on the word-bag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWNT2Wf3w5yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_list = [] # blank list to store the probabilities calculated via Naive Bayes\n",
        "\n",
        "for i in range(len(testing_text)): \n",
        "  sentences = testing_text[i].lower() # take the sentences from the testing set\n",
        "  \n",
        "  word_list = [] # another blank list to store the tokenized words from the \n",
        "  doc = nlp(sentences) # tokenize the sentences from testing set\n",
        "\n",
        "  for i in range(len(doc)): # for each token from the testing set\n",
        "    token = doc[i]\n",
        "    if token.is_alpha and not token.is_stop:\n",
        "      word_list.append(token.text) # Add it to the word_list if it's alphabetical and not a stop word\n",
        "\n",
        "  result = up.bayes_gothic(word_list, sorted_word_table, training_table) # Apply Naive Bayes to each word, comparing each to words in the word-bag and applying the Naive Bayes formula to calculate the probability that that word came from each author\n",
        "  result_list.append(result) # store the probabilities in the result_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx_xG9ZDxzkv",
        "colab_type": "text"
      },
      "source": [
        "`bayes_gothic` function from UO puddles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d-E2QAWx10O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bayes_gothic(evidence:list, evidence_bag:dframe, training_table:dframe, laplace:float=1.0) -> tuple:\n",
        "  assert isinstance(evidence, list), f'evidence not a list but instead a {type(evidence)}'\n",
        "  assert all([isinstance(item, str) for item in evidence]), f'evidence must be list of strings (not spacy tokens)'\n",
        "  assert isinstance(evidence_bag, pd.core.frame.DataFrame), f'evidence_bag not a dframe but instead a {type(evidence_bag)}'\n",
        "  assert isinstance(training_table, pd.core.frame.DataFrame), f'training_table not a dataframe but instead a {type(training_table)}'\n",
        "  assert 'author' in training_table, f'author column is not found in training_table'\n",
        "\n",
        "  author_list = training_table.author.unique().tolist()\n",
        "  mapping = ['EAP', 'MWS', 'HPL']\n",
        "  label_list = [mapping.index(auth) for auth in author_list]\n",
        "  n_classes = len(set(label_list))\n",
        "  #assert len(list(evidence_bag.values())[0]) == n_classes, f'Values in evidence_bag do not match number of unique classes ({n_classes}) in labels.'\n",
        "\n",
        "  word_list = evidence_bag.index.values.tolist()\n",
        "\n",
        "  evidence = list(set(evidence))  #remove duplicates\n",
        "  counts = []\n",
        "  probs = []\n",
        "  for i in range(n_classes):\n",
        "    ct = label_list.count(i)\n",
        "    counts.append(ct)\n",
        "    probs.append(ct/len(label_list))\n",
        "\n",
        "  #now have counts and probs for all classes\n",
        "\n",
        "  #CONSIDER CHANGING TO LN OF PRODUCTS. END UP SUMMING LOGS OF EACH ITEM. AVOIDS UNDERFLOW.\n",
        "  results = []\n",
        "  for a_class in range(n_classes):\n",
        "    numerator = 1\n",
        "    for ei in evidence:\n",
        "      if ei not in word_list:\n",
        "        #did not see word in training set\n",
        "        the_value =  1/(counts[a_class] + len(evidence_bag))\n",
        "      else:\n",
        "        values = evidence_bag.loc[ei].tolist()\n",
        "        the_value = ((values[a_class]+laplace)/(counts[a_class] + laplace*len(evidence_bag)))\n",
        "      numerator *= the_value\n",
        "    #if (numerator * probs[a_class]) == 0: print(evidence)\n",
        "    results.append(max(numerator * probs[a_class], 2.2250738585072014e-308))\n",
        "\n",
        "  return tuple(results)\n",
        "\n",
        "#used week 5 and moved here week 6\n",
        "def float_mult(number_list: list) -> float:\n",
        "  assert isinstance(number_list, list), f'number_list should be a list but is instead a {type(number_list)}'\n",
        "  assert all([isinstance(item, float) for item in number_list]), f'number_list must contain all floats'\n",
        "\n",
        "  result = 1.\n",
        "  for number in number_list:  #fancier version of for i in range(n):\n",
        "    result *= number\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK-gmntGyDMu",
        "colab_type": "text"
      },
      "source": [
        "# Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBe8nIzuyHvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "authors = ['EAP', 'MWS', 'HPL']\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(len(result_list)):\n",
        "  result = result_list[i] # Each result (list of probabilities for each author)\n",
        "\n",
        "  for i in range(len(result)):\n",
        "    m = max(result) # find the max probability \n",
        "    author_index = result.index(m) # get the (numerical?) index (author name) for each\n",
        "    author = authors[author_index] # find the author name corresponding to each index\n",
        "    pred = author # prediction is the author predicted for each row based on which author had the max probability for that word\n",
        "\n",
        "  predictions.append(pred) # fill the blank predictions list with the author predictions\n",
        "\n",
        "predictions[:10] # Predicted outcomes\n",
        "testing_authors[:10] # Compare to actual outcomes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgK50NOdyR8e",
        "colab_type": "text"
      },
      "source": [
        "# Calculate Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFh9wixiyTlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cases = list(zip(predictions,testing_authors))\n",
        "print(cases[:10])\n",
        "\n",
        "print(cases.count(('EAP', 'EAP')))\n",
        "print(cases.count(('MWS', 'MWS')))\n",
        "print(cases.count(('HPL', 'HPL')))\n",
        "\n",
        "accuracy = (2008 + 1467 + 1330)/len(testing_authors)\n",
        "accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TEc3KUxyar2",
        "colab_type": "text"
      },
      "source": [
        "# Heat Map (a way of visualizing accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWYBF-_syc9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "up.heat_map(cases, ['EAP', 'MWS', 'HPL'])  #EAP=0, MWS=1, HPL=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B233k2Ogye__",
        "colab_type": "text"
      },
      "source": [
        "# `heat_map` function from UO puddles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hN3Ly8Dyjmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def heat_map(zipped, label_list):\n",
        "  case_list = []\n",
        "  for i in range(len(label_list)):\n",
        "    inner_list = []\n",
        "    for j in range(len(label_list)):\n",
        "      inner_list.append(zipped.count((label_list[i], label_list[j])))\n",
        "    case_list.append(inner_list)\n",
        "\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 10))\n",
        "  ax.imshow(case_list)\n",
        "  ax.grid(False)\n",
        "  ax.set_xlabel('Predicted outputs', fontsize=32, color='black')\n",
        "  ax.set_ylabel('Actual outputs', fontsize=32, color='black')\n",
        "  ax.xaxis.set(ticks=range(len(label_list)))\n",
        "  ax.yaxis.set(ticks=range(len(label_list)))\n",
        "  \n",
        "  for i in range(len(label_list)):\n",
        "      for j in range(len(label_list)):\n",
        "          ax.text(j, i, case_list[i][j], ha='center', va='center', color='white', fontsize=32)\n",
        "  plt.show()\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKW70cJ7bGSu",
        "colab_type": "text"
      },
      "source": [
        "# Artificial Neural Nets (ANNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kt4wtOQcT2L",
        "colab_type": "text"
      },
      "source": [
        "## Calculate an ANN from scratch\n",
        "\n",
        "1. First, specify the input values & weights on the input values. Calculate the dot-product for these.\n",
        "2. Apply an activiation function to the result from 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un-L3uNdcW_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the input values and the weights on the input values\n",
        "inputs = [.002, -.09, .6] # values coming from a previous layer\n",
        "weights = [.5, .4, -.2] # weights on those values\n",
        "\n",
        "# Calculate the dot-product of the weights and input values\n",
        "z = dot(weights, inputs)\n",
        "z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSqLHyEIciB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining the activation functions\n",
        "\n",
        "## Sigmoid function\n",
        "def sigmoid(t:float) -> float:\n",
        "  s = 1 / (1 + math.exp(-t)) # e to the -t power\n",
        "  return s\n",
        "\n",
        "## RELU function\n",
        "def relu(t:float) -> float:\n",
        "  result = max(t, 0.0)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvUGoe1oc7ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the sigmoid function to the dot-product, z\n",
        "sigmoid(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU95UD_wc_NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the RELU function to the dot-product, z.\n",
        "relu(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsxTJ6HdC9d",
        "colab_type": "text"
      },
      "source": [
        "# `neuron_output` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaxbUZNDdEsJ",
        "colab_type": "text"
      },
      "source": [
        "A single function for computing the output node (using the sigmoid activation function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeH8LJURdLn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neuron_output(weights:list, inputs:list) -> float:\n",
        "  assert isinstance(weights, list), f'weights should be a list but is instead a {type(weights)}'\n",
        "  assert isinstance(inputs, list), f'inputs should be a list but is instead a {type(inputs)}'\n",
        "  assert len(weights) == len(inputs), f'weights and inputs should be the same length'\n",
        "\n",
        "  z = dot(weights, inputs)\n",
        "  s = sigmoid(z)\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uP2HkzSdO7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the neuron_output function out - it takes inputs and weights as arguments\n",
        "neuron_output(weights, inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0zV9e1EegPL",
        "colab_type": "text"
      },
      "source": [
        "# A feedforward function\n",
        "\n",
        "Takes as arguments the set of weights in a network and input values.\n",
        "\n",
        "Outputs the final result (i.e., prediction).\n",
        "\n",
        "<img src='https://codingvision.net/imgs/posts/c-backpropagation-tutorial-xor/1.png'>\n",
        "\n",
        "<img src='https://www.dropbox.com/s/fvko9fo71pp1cpr/Screenshot%202020-02-21%2009.14.37.png?raw=1'>\n",
        "\n",
        "\n",
        "Even though there are three layers shown above, the input layer is implied.\n",
        "- Only have to deal with two layers: the hidden and output\n",
        "\n",
        "#### Choosing the weights\n",
        "\n",
        "It's up to the researcher. One way is to use a random distribution of weights between -1 and 1. \n",
        "\n",
        "So for the first hidden node, we will have a list of two weights (notice two weights feed into it in the image above).\n",
        "\n",
        "<pre>\n",
        "hidden1 = [rdist1, rdist2]\n",
        "</pre>\n",
        "\n",
        "`rdist1` and `rdist2` are random numbers falling in a uniform distribution. We'll need the same for hidden2 and for the output node.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0wGdXXjhWsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting random numbers to use as the weights for the hidden nodes & output node\n",
        "np.random.seed(1234)\n",
        "\n",
        "hidden1 = list(np.random.uniform(-1,1,2)) # Create a list of 2 random items taken from a uniform distribution between -1 and 1\n",
        "hidden2 = list(np.random.uniform(-1,1,2))\n",
        "output = list(np.random.uniform(-1,1,2))\n",
        "\n",
        "\n",
        "# Combine the weights into a single object - a list of lists\n",
        "\n",
        "xor_network = [[hidden1, hidden2], [output]] # the weights for each node in the hidden layer are contained in a separate list from the weights for the output node\n",
        "\n",
        "print(len(xor_network)) # 2 (there are two separate lists)\n",
        "print(xor_network)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY936olVhggz",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/a8s43c314op5qg8/Screenshot%202020-05-13%2015.11.06.png?raw=1' height=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2bOE1hphmOM",
        "colab_type": "text"
      },
      "source": [
        "# `layer_output` function\n",
        "\n",
        "*   First, define the function `layer_output`.\n",
        "*   The `layer` parameter below is something like xor_network[0] or xor_network[1]\n",
        "*   The `inputs` are values from the preceding layer.\n",
        "*   You will use the 'create a new list from an old list' gist and the `neuron_output` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE0bQgejh4Ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def layer_output(layer:list, inputs:list) -> list:\n",
        "  assert isinstance(layer, list), f'layer must be a list but is a {type(layer)}'\n",
        "  assert all([isinstance(item, list) for item in layer]), f'layer must be a list of lists'\n",
        "  assert isinstance(inputs, list), f'inputs must be a list but is a {type(inputs)}'\n",
        "\n",
        "  new_list = []\n",
        "\n",
        "  for i in range(len(layer)):\n",
        "    item = layer[i]  \n",
        "    output = neuron_output(item, inputs)\n",
        "\n",
        "    new_list.append(output)\n",
        "\n",
        "  return new_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b7PI2lAiEAm",
        "colab_type": "text"
      },
      "source": [
        "# `feed_forward` function\n",
        "\n",
        "Create a function called `feed_forward` that takes the entire network in as a parameter (instead of each layer in steps), as well as the initial input to the network. \n",
        "\n",
        "It will go through each layer calling `layer_output`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CvlfeYbiFl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feed_forward(neural_network:list, input_vector:list) -> float:\n",
        "\n",
        "  outputs = []\n",
        "\n",
        "  for i in range(len(neural_network)):\n",
        "    layer = neural_network[i]  #layer\n",
        "    output = layer_output(layer, input_vector) # want to use the output from this as the input for next layer_output\n",
        "    outputs.append(output)\n",
        "    final_output = layer_output(layer, outputs[0])\n",
        "\n",
        "  return final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaZIu5I1k5sm",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Nets (CNNs)\n",
        "\n",
        "## on 1D data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jvMECfYk--2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There are standard settings used to load the data and split into training & testing sets:\n",
        "\n",
        "max_features = 5000\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 2\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrMgxe_nlSTq",
        "colab_type": "text"
      },
      "source": [
        "### Build the 300-D vectors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeTsl8QolWfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=maxlen))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn\n",
        "# word group filters of size filter_length:\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "# we use max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tABHr46lcdr",
        "colab_type": "text"
      },
      "source": [
        "# Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgYcSJvyldVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-P-U5AalfSG",
        "colab_type": "text"
      },
      "source": [
        "# Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzEKrxjdlgOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}