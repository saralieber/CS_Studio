{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Review_Ch8_ArtificialNeuralNets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO03a4kwC8uwuza5Yih+1nX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saralieber/CS_Studio/blob/master/Review_Ch8_ArtificialNeuralNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JSEDYU2bTi8",
        "colab_type": "text"
      },
      "source": [
        "# Artificial Neural Nets (ANNs)\n",
        "*   aka Multi-Layer Perceptron (MLP) \n",
        "*   aka Deep-Learning Model\n",
        "\n",
        "\n",
        "Neural networks were proposed by McCulloch and Pitts (1943) by likening a model to the neurons in the nervous sytem.\n",
        "*   ANNs - a complex computation made up of simple parts (i.e., neurons) where each simple part is only connected to its neighboring parts \n",
        "*   Each \"neuron\" has an input and an output; they are organized in layers\n",
        "\n",
        "\n",
        "The first neural nets were simple\n",
        "\n",
        "<img src='https://miro.medium.com/max/1032/1*WswH2fPx0bf_JFRMm8V-HA.gif'>\n",
        "\n",
        "This is just linear regression.\n",
        "\n",
        "\n",
        "\n",
        "The next proposed version of neural nets said add another layer.\n",
        "\n",
        "<img src='http://www.practicalai.io/wp-content/uploads/2017/06/admission-data-linear.png'  height=200>\n",
        "\n",
        "But this is still just linear regression where the weights (beta weights) are linear transformations of the previous inputs.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1016/1*g2HHjCkxeemizfLQC-BaAg.gif'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "There's nothing wrong with linear regression - just that it can only be used for problems that require linear solutions. For example, determining who survived and died from the Titanic data set can be solved using a linear regression model.\n",
        "\n",
        "<img src='https://sebastianraschka.com/images/faq/logistic_regression_linear/4.png'>\n",
        "\n",
        "\n",
        "\n",
        "But what is there is no line that cleanly separates the data?\n",
        "\n",
        "<img src='https://www.dropbox.com/s/rk83o8qqgr7vsq2/Screenshot%202020-04-28%2011.00.53.png?raw=1'>\n",
        "\n",
        "\n",
        "\n",
        "That's where other algorithms come in, like...\n",
        "\n",
        "## Support Vector Machines (SVM)\n",
        "\n",
        "See the image above. What an SVM does is takes a 2D space and maps it onto 3D space. (The opposite of feature reduction - it's feature expansion!)\n",
        "\n",
        "Once in 3D space, you can find a plane that may cleanly separate the points you're trying to classify. First, you have to find the 2D-to-3D mapping function, called a *kernel function*. Then, you have to find the plane. \n",
        "\n",
        "More to come on SVM later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZyyY_zsbd70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4TfQoIDd0ZG",
        "colab_type": "text"
      },
      "source": [
        "## Artificial Neural Nets (ANNs) \n",
        "\n",
        "ANNs are another alternative to SVM. They end up being more versatile to work with than SVM when it comes to working with text data.\n",
        "\n",
        "The jump in our understanding of neural nets came from observing biological neurons and seeing that the cell body takes a summation of inputs, and these inputs have to surpass a certain threshold to active the neuron. \n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/4/44/Neuron3.png'>\n",
        "\n",
        "Leads to the idea of an *activation function* - a certain thredhold has to be reached before firing takes place.\n",
        "\n",
        "\n",
        "\n",
        "This is what a simple ANN looks like:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/zdv3sjzssiewwf3/Screenshot%202020-02-18%2010.30.12.png?raw=1'>\n",
        "\n",
        "The nodes on the left make up the *input layer*.\n",
        "- This is a vector that we pass into the model. It represents one row of a dataset being handed to the ANN. \n",
        "- So 0.44 would be the first number in the vector, 0.33 would be the second, and so on.\n",
        "- The vectors we'll work with are *dense*. This means each node in the input layer supplies a value to each node in the hidden layer (note the lines from every input node to every node in the hidden layer).\n",
        "\n",
        "The nodes in the middle make up the *hidden layer* (not actually hidden).\n",
        "- The nodes in the hidden layer are connected to all the nodes in the output layer (a single node, in this case). \n",
        "\n",
        "Not depicted in this picture are the weights - there should be a weight corresponding to each line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PTDZ99iiLvY",
        "colab_type": "text"
      },
      "source": [
        "## Nodes\n",
        "\n",
        "The circles in the diagrams above are called nodes. Each node has k input lines. The outliers (?) are the input nodes with only 1 input line and no weights. They just pass through the value from the sample to the first hidden layer.\n",
        "\n",
        "\n",
        "<img src='https://www.researchgate.net/profile/Karem_Chokmani/publication/255629329/figure/fig2/AS:339705213276163@1458003442306/The-basic-element-of-a-neural-network-node-computation.png'>\n",
        "\n",
        "\n",
        "\n",
        "In this diagram,\n",
        "- The inputs to the node are labeled O (to represent that they are the output from the previous layer to the left)\n",
        "- Each input line has its own weight, W\n",
        "- The operation of the first part of the node is the dot-product of the outputs (vector 1) and the weights (vector 2), where the resulting dot-product is typically called Z. This is represented by the function labeled I-sub-j, where j is the node number.\n",
        "- The activation function is labeled f (this function was labeled A in an earlier diagram). It takes the result of the dot-product (Z) and produces the actual output of the node. If you choose a linear activiation function (e.g., f(x) = cx), you'll end up with a network that computes a linear function no matter how many layers and nodes you have.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLAdqh7LjlwO",
        "colab_type": "text"
      },
      "source": [
        "## Exclusive-OR\n",
        "\n",
        "We'll use the XOR function for the following examples. The XOR function has an output of True if only one of its inputs is True, but not both.\n",
        "\n",
        "\n",
        "The possible outcomes of the XOR function are:\n",
        "- Inputs: True and True, Output: False\n",
        "- Inputs: True and False, Output: True\n",
        "- Inputs: False and True, Output: True\n",
        "- Inputs: False and False, Output: False\n",
        "\n",
        "\n",
        "\n",
        "Here are these outcomes represented graphically:\n",
        "\n",
        "<img src='https://www.dropbox.com/s/oeud4lstd84l88d/Screenshot%202020-02-21%2013.27.25.png?raw=1' height=300>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "There's not a linear solution to this program. The solution? use a non-linear activation function.\n",
        "\n",
        "\n",
        "Here are a couple of popular non-linear activation functions.\n",
        "\n",
        "## 1. RELU\n",
        "\n",
        "RELU stands for rectified linear activation function. It is a piecewise linear function that will output the input directly if the input is positive; otherwise, it will output zero (if the input is negative). The function in python for it is `max(0,x)`.\n",
        "\n",
        "<img src='https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png' height=200>\n",
        "\n",
        "\n",
        "This might be the closest function to mimicking a bioloigcal neuron that needs to reach a threshold before activating.\n",
        "\n",
        "\n",
        "## 2. Sigmoid\n",
        "\n",
        "Sigmoid produces an \"S\" curve with the following function (the logistic version produces a value between 0 and 1):\n",
        "\n",
        "<img src='https://www.dropbox.com/s/58hr9e4iusnmapc/Screenshot%202020-02-18%2014.02.06.png?raw=1'>\n",
        "\n",
        "<img src='https://www.dropbox.com/s/wdqdl22m2l7jruo/Screenshot%202020-02-18%2014.02.21.png?raw=1'>\n",
        "\n",
        "\n",
        "## 3. Others\n",
        "\n",
        "See https://medium.com/@himanshuxd/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e for a nice table of activation functions you have to choose from.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoRyg_ALmWAi",
        "colab_type": "text"
      },
      "source": [
        "## How is ANN implemented?\n",
        "\n",
        "What is a node? It really is its vector of weights on the outputs from the preceding layer. \n",
        "\n",
        "So if we store all our node weights in a matrix (one matrix per layer - input layer, hidden layer), then we are just doing a dot-product of the output vector against the weight matrix. \n",
        "\n",
        "This will produce an intermediate value (called Z). Z gets passed to the activation function to get the node output.\n",
        "\n",
        "\n",
        "Example: Build an ANN from scratch using the XOR function as an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_bPVdBm5XF",
        "colab_type": "code",
        "outputId": "03de3066-7aee-4acc-808a-3ad1da5f5d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# Import Steve's uo_puddles library\n",
        "!rm -r  'uo_puddles'\n",
        "my_github_name = 'uo-puddles'  \n",
        "clone_url = f'https://github.com/{my_github_name}/uo_puddles.git'\n",
        "!git clone $clone_url \n",
        "import uo_puddles.uo_puddles as up"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'uo_puddles': No such file or directory\n",
            "Cloning into 'uo_puddles'...\n",
            "remote: Enumerating objects: 258, done.\u001b[K\n",
            "remote: Counting objects: 100% (258/258), done.\u001b[K\n",
            "remote: Compressing objects: 100% (222/222), done.\u001b[K\n",
            "remote: Total 258 (delta 154), reused 64 (delta 33), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (258/258), 66.97 KiB | 387.00 KiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDrChB0wnFYj",
        "colab_type": "text"
      },
      "source": [
        "### Build a function that calculates dot-products\n",
        "\n",
        "Reminder: the dot-product multiplies corresponding items from two vectors, then takes the sum of the products."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgA_RmqWnSDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the dot-product of two vectors\n",
        "\n",
        "def dot(vector1: list, vector2:list) -> float:\n",
        "  assert isinstance(vector1, list), f'vector1 should be a list but is instead a {type(vector1)}'\n",
        "  assert isinstance(vector2, list), f'vector2 should be a list but is instead a {type(vector2)}'\n",
        "  assert len(vector1) == len(vector2), f'both vectors should be the same length'\n",
        "\n",
        "  result = 0\n",
        "  for i in range(len(vector1)):\n",
        "    term = vector1[i]*vector2[i]\n",
        "    result += term\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esG1vuTfpqxX",
        "colab_type": "code",
        "outputId": "c0df7a2e-d4ef-4664-9716-f6b78622272b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Test the dot function\n",
        "inputs = [.002, -.09, .6] # values coming from a previous layer\n",
        "weights = [.5, .4, -.2] # weights on those values\n",
        "\n",
        "z = dot(weights, inputs) # why weights first? does it matter?\n",
        "z # This is I-sub-j in the diagram below"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNC5uJOJqHi3",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/en3wtc2p8nea5ig/Screenshot%202020-05-13%2014.35.58.png?raw=1' height=400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Bf5EGbqUuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, we need to apply an activiation function to z, f(z)\n",
        "## You get to choose which activation function to use. Let's define and use a couple."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN5u_ZVkqeho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sigmoid function\n",
        "import math\n",
        "\n",
        "def sigmoid(t:float) -> float:\n",
        "  s = 1 / (1 + math.exp(-t)) # e to the -t power\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8sro3rFqmyr",
        "colab_type": "code",
        "outputId": "381b8a38-8c33-4270-bbad-fdfeb4d9e63d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Apply the sigmoid function to the dot-product, z\n",
        "sigmoid(z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46132739479349205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QzoaGRRqvVJ",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/exe1xq59vpnxfrl/Screenshot%202020-05-13%2014.46.43.png?raw=1' height=400>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNjoMo31q0Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now try the RELU activation function."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45x543Uxq26W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RELU function\n",
        "def relu(t:float) -> float:\n",
        "  result = max(t, 0.0)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72SoH6XOq8L8",
        "colab_type": "code",
        "outputId": "591a338e-81f6-49bb-b079-7ac8633fbd3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Apply the RELU function to the dot-product, z.\n",
        "relu(z)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4hbDA0wrDEq",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/wv16f1ym79gqmzw/Screenshot%202020-05-13%2014.49.58.png?raw=1' height=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Teo3vQYTrUrW",
        "colab_type": "text"
      },
      "source": [
        "#### Now, write a single function for computing the output node assuming we want to use the sigmoid activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xHjCwe9rFiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neuron_output(weights:list, inputs:list) -> float:\n",
        "  assert isinstance(weights, list), f'weights should be a list but is instead a {type(weights)}'\n",
        "  assert isinstance(inputs, list), f'inputs should be a list but is instead a {type(inputs)}'\n",
        "  assert len(weights) == len(inputs), f'weights and inputs should be the same length'\n",
        "\n",
        "  z = dot(weights, inputs)\n",
        "  s = sigmoid(z)\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NePdFFXXrinr",
        "colab_type": "code",
        "outputId": "6a301b43-826b-4590-8bc5-379e999ca3dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Test the neuron_output function out.\n",
        "neuron_output(weights, inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46132739479349205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD6nVJsZrn6V",
        "colab_type": "text"
      },
      "source": [
        "## A feedforward function\n",
        "\n",
        "A feedforward function will take as arguments the set of weights in a network and input values. It will output the final result (i.e., prediction).\n",
        "\n",
        "Example using the XOR function. The feedforward function takes two binary numbers in and outputs a value between 0 and 1. Take the round of the result to get a binary number as a prediction:\n",
        "\n",
        "<img src='https://codingvision.net/imgs/posts/c-backpropagation-tutorial-xor/1.png'>\n",
        "\n",
        "<img src='https://www.dropbox.com/s/fvko9fo71pp1cpr/Screenshot%202020-02-21%2009.14.37.png?raw=1'>\n",
        "\n",
        "\n",
        "Even though there are three layers shown above, the input layer is implied.\n",
        "- Only have to deal with two layers: the hidden and output\n",
        "- The input layer just pumps data to the first hideen layer, so it is implied\n",
        "- Think of this as the weights belonging to the layer on the right. Given the input layer has no weights on its left, we can leave it out. Essentially, it is represented as the `input_vector` below.\n",
        "\n",
        "\n",
        "#### Choosing the weights\n",
        "\n",
        "It's up to the researcher. One way is to use a random distribution of weights between -1 and 1. \n",
        "\n",
        "So for the first hidden node, we will have a list of two weights (notice two weights feed into it in the image above).\n",
        "\n",
        "<pre>\n",
        "hidden1 = [rdist1, rdist2]\n",
        "</pre>\n",
        "\n",
        "`rdist1` and `rdist2` are random numbers falling in a uniform distribution. We'll need the same for hidden2 and for the output node.\n",
        "\n",
        "There are a total of three nodes in the network shown above, and each needs a list of its own weights. We're randomly gathering these weights below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbM_B-Vbtv-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "\n",
        "hidden1 = list(np.random.uniform(-1,1,2)) # Create a list of 2 random items taken from a uniform distribution between -1 and 1\n",
        "hidden2 = list(np.random.uniform(-1,1,2))\n",
        "output = list(np.random.uniform(-1,1,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l08ym9XeuGfL",
        "colab_type": "code",
        "outputId": "e54a9408-be69-4ab9-8aab-409a3c606645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Combine the weights into a single object - a list of lists\n",
        "\n",
        "xor_network = [[hidden1, hidden2], [output]] # the weights for each node in the hidden layer are contained in a separate list from the weights for the output node\n",
        "\n",
        "print(len(xor_network)) # 2 (there are two separate lists)\n",
        "print(xor_network)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "[[[-0.6169610992422154, 0.24421754207966373], [-0.12454452198577104, 0.5707171674275384]], [[0.559951616237607, -0.45481478943471676]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEzigpc4uib0",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/a8s43c314op5qg8/Screenshot%202020-05-13%2015.11.06.png?raw=1' height=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXL82OqicDcT",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29bGRZ6vuXfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assignment 1\n",
        "## Complete the last two functions we need to simulate a feedforward network\n",
        "\n",
        "# First, define the function `layer_output`.\n",
        "## The `layer` parameter below is something like xor_network[0] or xor_network[1]\n",
        "## The `inputs` are values from the preceding layer.\n",
        "## You will use the 'create a new list from an old list' gist and the `neuron_output` function."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XTKCQj6vBLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def layer_output(layer:list, inputs:list) -> list:\n",
        "  assert isinstance(layer, list), f'layer must be a list but is a {type(layer)}'\n",
        "  assert all([isinstance(item, list) for item in layer]), f'layer must be a list of lists'\n",
        "  assert isinstance(inputs, list), f'inputs must be a list but is a {type(inputs)}'\n",
        "\n",
        "  new_list = []\n",
        "\n",
        "  for i in range(len(layer)):\n",
        "    item = layer[i]  \n",
        "    output = neuron_output(item, inputs)\n",
        "\n",
        "    new_list.append(output)\n",
        "\n",
        "  return new_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxU8T1Mdc8fC",
        "colab_type": "text"
      },
      "source": [
        "I'll test your function out by stepping through the layers, left to right. First up is the hidden layer or `xor_network[0]`.\n",
        "\n",
        "We need a sample input. The xor operator takes binary value pairs. There are 4 such unique pairs, right? Let's just choose one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcyrLT83bCGz",
        "colab_type": "code",
        "outputId": "7e5f2caa-2c55-4b81-a99a-6c4df5bc4d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "layer = xor_network[0] # the hidden layer\n",
        "input = [0,1]\n",
        "print(layer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.6169610992422154, 0.24421754207966373], [-0.12454452198577104, 0.5707171674275384]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMPNQoQNc_3v",
        "colab_type": "text"
      },
      "source": [
        "Now produce the output for the first layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnMct-4scxVr",
        "colab_type": "code",
        "outputId": "88056a62-e7c6-4bad-ff6b-a151032a8642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "hidden_output = layer_output(layer, input)\n",
        "hidden_output # two outputs because two ndoes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5607527329852497, 0.6389286413163151]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_j_bSA-dJsK",
        "colab_type": "text"
      },
      "source": [
        "There are 2 neurons in the hidden layer so we get an output from each.\n",
        "\n",
        "Now here is the cool part. We take the output and make it the input to the 2nd layer, i.e., the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnqJxAbwdKZ6",
        "colab_type": "code",
        "outputId": "62d0196f-479c-46db-fccb-937cee0b8de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "input = hidden_output\n",
        "layer = xor_network[1] # the output layer\n",
        "print(layer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.559951616237607, -0.45481478943471676]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y2WIAaFdYC6",
        "colab_type": "text"
      },
      "source": [
        "Now ready to produce next output, which is the actual output of the entire network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDkcHSZidXQM",
        "colab_type": "code",
        "outputId": "56284c6e-e200-45ac-d918-201f8cf0947a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "output_output = layer_output(layer, input)\n",
        "output_output # prediction = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5058497839923097]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wzxOysWiNBc",
        "colab_type": "code",
        "outputId": "b69259d4-ce34-4389-aa37-1379a244fa42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "round(output_output[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5li5WlILdocZ",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "Create a function called `feed_forward` that takes the entire network in as a parameter (instead of each layer in steps), as well as the initial input to the network. \n",
        "\n",
        "It will go through each layer calling `layer_output`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx7j4xWPfepT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feed_forward(neural_network:list, input_vector:list) -> float:\n",
        "\n",
        "  outputs = []\n",
        "\n",
        "  for i in range(len(neural_network)):\n",
        "    layer = neural_network[i]  #layer\n",
        "    output = layer_output(layer, input_vector) # want to use the output from this as the input for next layer_output\n",
        "    outputs.append(output)\n",
        "    final_output = layer_output(layer, outputs[0])\n",
        "\n",
        "  return final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swpsmLsoeQl_",
        "colab_type": "text"
      },
      "source": [
        "Try it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZRf6J8PeVtH",
        "colab_type": "code",
        "outputId": "cc8ffa0d-1014-4a7f-b41a-a8bebf4bdc1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "raw_prediction = feed_forward(xor_network, [0,1])\n",
        "raw_prediction  #should be same as above: [0.5058497839923097]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5058497839923097]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YisCcx_tky4U",
        "colab_type": "text"
      },
      "source": [
        "We now have an ANN that's set up to compute the final predictions for the XOR problem.\n",
        "\n",
        "But, we're missing our label column. We need the actual labels (answers) to see how accurate our predictions were. We'll put off generating a label column for now - it's easier to use the XOR operator built into Python to get the actual results.\n",
        "\n",
        "Let's run through all 4 samples (inputs), get a prediction, call XOR for the actual answer, and then compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIlS6VxDl6Xo",
        "colab_type": "code",
        "outputId": "fb53c01f-ae5d-485d-90a4-2c6c8af8a41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "0^1 # this is the built-in python XOR function (\"hat\" is XOR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNWvbiR0mA5C",
        "colab_type": "text"
      },
      "source": [
        "Test on all XOR combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgp-NmP5m1rh",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://www.dropbox.com/s/fvko9fo71pp1cpr/Screenshot%202020-02-21%2009.14.37.png?raw=1'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7TUzrcNmCs4",
        "colab_type": "code",
        "outputId": "876eb328-6f23-4bc6-c5ef-3c516a656b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "for x in [0,1]: \n",
        "  for y in [0,1]:\n",
        "    result = feed_forward(xor_network,[x,y])\n",
        "    print(x,y, round(result[0]), x^y, result) \n",
        "    # prints each combination of numbers from x & y (first two columns)\n",
        "    # then prints the predicted result (third column)\n",
        "    # then prints the actual result (fourth column)\n",
        "    # then prints the raw result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 1 0 [0.5131390777373889]\n",
            "0 1 1 1 [0.5058497839923097]\n",
            "1 0 0 1 [0.4957459470865491]\n",
            "1 1 0 0 [0.4877720338005447]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzVhrfxtmUMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in [0,1]:\n",
        "  for y in [0,1]:\n",
        "    result = [x,y]\n",
        "    print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKTs4pXSmoZ4",
        "colab_type": "text"
      },
      "source": [
        "[0, 0]\n",
        "[0, 1]\n",
        "[1, 0]\n",
        "[1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5pwxAVEm5LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "0 0 1 0 [0.5131390777373889]  # x, y, predicted, actual (WRONG)\n",
        "0 1 1 1 [0.5058497839923097]  # x, y, predicted, actual (RIGHT)\n",
        "1 0 0 1 [0.4957459470865491]  # x, y, predicted, actual (WRONG)\n",
        "1 1 0 0 [0.4877720338005447]  # x, y, predicted, actual (RIGHT)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqcEzuQKnbp5",
        "colab_type": "text"
      },
      "source": [
        "We got 50% accuracy using randomly assigned weights. The outputs were always very close to .5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgi-X3Uynkv2",
        "colab_type": "text"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "The process we went through above really is similar to an actual neural network algorithm in how we used a list of weights and moved layer by layer toward the output. \n",
        "\n",
        "The difference between what we did above and a fully-developed set of ANN algorithms is that the latter tried to speed things up. It'll use a GPU to run calculations in parallel, and it'll use faster forms of the `dot` and `sigmoid` functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDF5K7njn_vA",
        "colab_type": "text"
      },
      "source": [
        "## What about the \"learning\" part of neural networks?\n",
        "\n",
        "We generated random weights above and then ran our four samples (inputs) to get four predictions for the XOR problem. But we're not getting very good results - all predictions were around .5. \n",
        "\n",
        "What we want is for the network to be able to learn from its mistakes.\n",
        "\n",
        "Let's say our network predicts .1 but the actual value is 1. REALLY big error (difference of .9).\n",
        "\n",
        "Our goal is to reduce the amount of error between predicted and actual. What do we have to change to achieve this? The weights. The weights are knobs we can turn."
      ]
    }
  ]
}