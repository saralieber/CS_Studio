{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Review_Ch7_Part2_Text_to_Vectors_Meanings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMc276E/ks0cvMY7dITh+EJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saralieber/CS_Studio/blob/master/Review_Ch7_Part2_Text_to_Vectors_Meanings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLRBbsYgD04i",
        "colab_type": "text"
      },
      "source": [
        "# Deriving meaning from words\n",
        "\n",
        "Is there a way to represent all English words so they have this \"closer in space is closer in meaning\" property that we saw with colors represented as their RGB properties?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea-090x6ENr6",
        "colab_type": "text"
      },
      "source": [
        "To answer this, we have to first think of what *meaning* means.\n",
        "\n",
        "One theory (Distributional Hypothesis) popular among computational linguists is that linguistic items with similar contexts have similar meanings.\n",
        "\n",
        "In other words, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhNQX4DwEuYQ",
        "colab_type": "text"
      },
      "source": [
        "How do we turn this insight into a system for creating general-purpose vectors that capture the meaning of words?\n",
        "\n",
        "Let's use a small source text to begin with, such as this except from Dickens:\n",
        "\n",
        "    It was the best of times, it was the worst of times.\n",
        "\n",
        "This spreadsheet tries to capture the context of words. \n",
        "![dickens contexts](http://static.decontextualize.com/snaps/best-of-times.png)\n",
        "\n",
        "The spreadsheet has one column for every possible context, and one row for every word. The values in each cell correspond with how many times the word occurs in the given context. The numbers in the columns constitute that word's vector, i.e., the vector for the word `of` is\n",
        "\n",
        "    [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwSw9eKKFMJX",
        "colab_type": "text"
      },
      "source": [
        "You could use the same distance formula we defined before to get useful information about which vectors in this space are similar to each other.\n",
        "\n",
        "In particular, the vectors for `best` and `worst` are actually the same (a distance of zero), since they occur only in the same context (`the __ of`).\n",
        "\n",
        "Of course, the conventional way of thinking about \"best\" and \"worst\" is that they're *antonyms*, not *synonyms*. But they're also clearly two words of the same kind, with related meanings (through opposition), a fact that is captured by this distributional model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8whC8UHFeh",
        "colab_type": "text"
      },
      "source": [
        "In many texts, there will be many thousands if not millions of possible contexts. It turns out, though, that many of these dimensions (contexts) will end up being superfluous and can either be eliminated or combined with other dimensions without significantly affecting the predictive power of the resulting vectors.\n",
        "\n",
        "The process of getting rid of superfluous dimensions in a vector space is called *dimensionality reduction*.\n",
        "\n",
        "The question of how to identify a \"context\" is difficult to answer. \n",
        "\n",
        "You might want to...\n",
        "\n",
        "*   Use the word before and after the given word (e.g., see example above)\n",
        "*   Use a larger window (e.g., the two words before and after the given word)\n",
        "*   Use a non-contiguous window (e.g., skip a word before and after the given word)\n",
        "*   Look at larger syntactic structure: what are the syntactic-contexts you find the word in?\n",
        "*   Exclude certain \"function\" words like \"the\" and \"of\" \n",
        "*   Lemmatize the words before you begin your analysis so two occurences with different \"forms\" of the same word count as the same context\n",
        "\n",
        "These are all questions open to research and debate.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRpLuLlEJHGf",
        "colab_type": "text"
      },
      "source": [
        "## GloVe vectors\n",
        "\n",
        "You don't have to create your own word vectors from scratch. Many researchers have made downloadable databases of pre-trained vectors.\n",
        "\n",
        "One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). These 300-dimensional vectors are included with spaCy, and they're the vectors we'll use for this activity. They come with `en_core_web_md`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO852lfOEAj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_md # download the dictionary\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOIzIhFVJsTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cc79b210-0b22-4cc0-d7ff-72b32e5cf01e"
      },
      "source": [
        "nlp.vocab.has_vector('frankenstein') # Check to make sure word vectors have been loaded"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snayYSSbJl7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dogv = nlp.vocab['dog'].vector # get the 300-dimensional vector for dog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uve-XtPYJ1eA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f1f32e5b-0999-4eac-be2b-859cdc1b2a02"
      },
      "source": [
        "type(dogv)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1JljkYfKEuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dog_list = dogv.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fIclThxKGrs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8ce432e1-9080-4eee-8458-9026e40e7be7"
      },
      "source": [
        "len(dog_list) # 300"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxBf-338KImU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "963242f9-b0cb-4a39-9bb7-79f0fc97faad"
      },
      "source": [
        "dog_list[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.4017600119113922,\n",
              " 0.37057000398635864,\n",
              " 0.02128100022673607,\n",
              " -0.3412500023841858,\n",
              " 0.04953800141811371,\n",
              " 0.29440000653266907,\n",
              " -0.17375999689102173,\n",
              " -0.2798199951648712,\n",
              " 0.06762199848890305,\n",
              " 2.169300079345703]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EO--DOhKMBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following function gets a vector of a given string from spaCy's vocabulary\n",
        "\n",
        "def get_vec(s:str) -> list:\n",
        "  return nlp.vocab[s].vector.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMIBVH4ZKUlA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "884b4ae9-7efd-403e-adb1-72e8c5e6c813"
      },
      "source": [
        "get_vec('dog') == dog_list # should be True"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owb6xtSCKYu5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d27d050f-f362-4597-b1da-eb6cf1e0600d"
      },
      "source": [
        "# There is also a vector for words not in the vocab. It is all zeroes.\n",
        "\n",
        "zero_vec = get_vec('askfsda') # Not in vocab\n",
        "zero_vec.count(0) # 300 zeroes, i.e., all zeroes"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMz5WOV4Ki0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "cbad05b3-01dd-4009-83ec-72ae9aa48f0a"
      },
      "source": [
        "# The following shows that cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`\n",
        "\n",
        "up.cosine_similarity(get_vec('dog'), get_vec('puppy')) > up.cosine_similarity(get_vec('trousers'), get_vec('octopus'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d6b33e242fc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The following shows that cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'puppy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trousers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'octopus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'up' is not defined"
          ]
        }
      ]
    }
  ]
}