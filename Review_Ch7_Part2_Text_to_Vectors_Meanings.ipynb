{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Review_Ch7_Part2_Text_to_Vectors_Meanings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRLh9YCZdmU+YFfKSTudfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saralieber/CS_Studio/blob/master/Review_Ch7_Part2_Text_to_Vectors_Meanings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLRBbsYgD04i",
        "colab_type": "text"
      },
      "source": [
        "# Deriving meaning from words\n",
        "\n",
        "Is there a way to represent all English words so they have this \"closer in space is closer in meaning\" property that we saw with colors represented as their RGB properties?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea-090x6ENr6",
        "colab_type": "text"
      },
      "source": [
        "To answer this, we have to first think of what *meaning* means.\n",
        "\n",
        "One theory (Distributional Hypothesis) popular among computational linguists is that linguistic items with similar contexts have similar meanings.\n",
        "\n",
        "In other words, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhNQX4DwEuYQ",
        "colab_type": "text"
      },
      "source": [
        "How do we turn this insight into a system for creating general-purpose vectors that capture the meaning of words?\n",
        "\n",
        "Let's use a small source text to begin with, such as this except from Dickens:\n",
        "\n",
        "    It was the best of times, it was the worst of times.\n",
        "\n",
        "This spreadsheet tries to capture the context of words. \n",
        "![dickens contexts](http://static.decontextualize.com/snaps/best-of-times.png)\n",
        "\n",
        "The spreadsheet has one column for every possible context, and one row for every word. The values in each cell correspond with how many times the word occurs in the given context. The numbers in the columns constitute that word's vector, i.e., the vector for the word `of` is\n",
        "\n",
        "    [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwSw9eKKFMJX",
        "colab_type": "text"
      },
      "source": [
        "You could use the same distance formula we defined before to get useful information about which vectors in this space are similar to each other.\n",
        "\n",
        "In particular, the vectors for `best` and `worst` are actually the same (a distance of zero), since they occur only in the same context (`the __ of`).\n",
        "\n",
        "Of course, the conventional way of thinking about \"best\" and \"worst\" is that they're *antonyms*, not *synonyms*. But they're also clearly two words of the same kind, with related meanings (through opposition), a fact that is captured by this distributional model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8whC8UHFeh",
        "colab_type": "text"
      },
      "source": [
        "In many texts, there will be many thousands if not millions of possible contexts. It turns out, though, that many of these dimensions (contexts) will end up being superfluous and can either be eliminated or combined with other dimensions without significantly affecting the predictive power of the resulting vectors.\n",
        "\n",
        "The process of getting rid of superfluous dimensions in a vector space is called *dimensionality reduction*.\n",
        "\n",
        "The question of how to identify a \"context\" is difficult to answer. \n",
        "\n",
        "You might want to...\n",
        "\n",
        "*   Use the word before and after the given word (e.g., see example above)\n",
        "*   Use a larger window (e.g., the two words before and after the given word)\n",
        "*   Use a non-contiguous window (e.g., skip a word before and after the given word)\n",
        "*   Look at larger syntactic structure: what are the syntactic-contexts you find the word in?\n",
        "*   Exclude certain \"function\" words like \"the\" and \"of\" \n",
        "*   Lemmatize the words before you begin your analysis so two occurences with different \"forms\" of the same word count as the same context\n",
        "\n",
        "These are all questions open to research and debate.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRpLuLlEJHGf",
        "colab_type": "text"
      },
      "source": [
        "## GloVe vectors\n",
        "\n",
        "You don't have to create your own word vectors from scratch. Many researchers have made downloadable databases of pre-trained vectors.\n",
        "\n",
        "One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). These 300-dimensional vectors are included with spaCy, and they're the vectors we'll use for this activity. They come with `en_core_web_md`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO852lfOEAj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_md # download the dictionary\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOIzIhFVJsTh",
        "colab_type": "code",
        "outputId": "4a66a08e-7180-40ff-f405-d7f1ba2d4cc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nlp.vocab.has_vector('frankenstein') # Check to make sure word vectors have been loaded"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snayYSSbJl7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dogv = nlp.vocab['dog'].vector # get the 300-dimensional vector for dog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uve-XtPYJ1eA",
        "colab_type": "code",
        "outputId": "aae39f7a-4eb2-4415-a567-cc6bad94d765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(dogv)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1JljkYfKEuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dog_list = dogv.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fIclThxKGrs",
        "colab_type": "code",
        "outputId": "ec10902f-5bfe-4167-c25a-177d4ead4926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(dog_list) # 300"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxBf-338KImU",
        "colab_type": "code",
        "outputId": "2d936457-6dfe-4adf-e4e1-bc606d41a873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "dog_list[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.4017600119113922,\n",
              " 0.37057000398635864,\n",
              " 0.02128100022673607,\n",
              " -0.3412500023841858,\n",
              " 0.04953800141811371,\n",
              " 0.29440000653266907,\n",
              " -0.17375999689102173,\n",
              " -0.2798199951648712,\n",
              " 0.06762199848890305,\n",
              " 2.169300079345703]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EO--DOhKMBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following function gets a vector of a given string from spaCy's vocabulary\n",
        "\n",
        "def get_vec(s:str) -> list:\n",
        "  return nlp.vocab[s].vector.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMIBVH4ZKUlA",
        "colab_type": "code",
        "outputId": "a3ff80aa-c30b-4a2e-b128-99556b6aa318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "get_vec('dog') == dog_list # should be True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owb6xtSCKYu5",
        "colab_type": "code",
        "outputId": "13bf3d58-f510-4ca4-a7fe-98567e665a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# There is also a vector for words not in the vocab. It is all zeroes.\n",
        "\n",
        "zero_vec = get_vec('askfsda') # Not in vocab\n",
        "zero_vec.count(0) # 300 zeroes, i.e., all zeroes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeT9ynIaLFGB",
        "colab_type": "code",
        "outputId": "3224d902-1dac-4804-fdc0-c704c27def6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "# Import uo_puddles library\n",
        "\n",
        "!rm -r 'uo_puddles'\n",
        "my_github_name = 'uo-puddles' # can replace with your account name\n",
        "clone_url = f'https://github.com/{my_github_name}/uo_puddles.git'\n",
        "!git clone $clone_url\n",
        "import uo_puddles.uo_puddles as up"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'uo_puddles': No such file or directory\n",
            "Cloning into 'uo_puddles'...\n",
            "remote: Enumerating objects: 234, done.\u001b[K\n",
            "remote: Counting objects: 100% (234/234), done.\u001b[K\n",
            "remote: Compressing objects: 100% (198/198), done.\u001b[K\n",
            "remote: Total 234 (delta 139), reused 64 (delta 33), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (234/234), 59.83 KiB | 2.99 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMz5WOV4Ki0Z",
        "colab_type": "code",
        "outputId": "f89f97e9-3172-4e2f-be18-740e61fa044b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# The following shows that cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`\n",
        "\n",
        "up.cosine_similarity(get_vec('dog'), get_vec('puppy')) > up.cosine_similarity(get_vec('trousers'), get_vec('octopus'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcPOITkYLnUg",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Similarity\n",
        "\n",
        "Moving toward doing prediction. We will be interested in converting an entire sentence into a single glove vector.\n",
        "\n",
        "Here is the general idea of how we will do this. First, go through each row and grab the text of the sentence. Build a list of guarded tokens (kind of like e_list for Naive Bayes).\n",
        "\n",
        "Then, get the vectors for all the tokens and build a matrix. Take the average. You can use that vector average as the representation of the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVFvx7KfL5AY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "\n",
        "pilot_sentences = [\n",
        "  'It was really cold yesterday.',\n",
        "  'It will be really warm today, though.',\n",
        "  \"It'll be really hot tomorrow!'\",\n",
        "  'Will it be really cool Tuesday?'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paE5sPxgNQPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def addv(x:list, y:list) -> list: # define a function, called addv, that takes variables x & y (both lists) as inputs\n",
        "  assert isinstance(x, list), f\"x must be a list but instead is {type(x)}\"\n",
        "  assert isinstance(y, list), f\"y must be a list but instead is {type(y)}\"\n",
        "  assert len(x) == len(y), f\"x and y must be the same length\"\n",
        "\n",
        "  #result = [(c1 - c2) for c1, c2 in zip(x, y)]  #one-line compact version - called a list comprehension\n",
        "\n",
        "  result = []\n",
        "  for i in range(len(x)):\n",
        "    c1 = x[i]\n",
        "    c2 = y[i]\n",
        "    result.append(c1+c2)\n",
        "\n",
        "  return result\n",
        "\n",
        "def dividev(x:list, y:int) -> list:\n",
        "  assert isinstance(x, list), f\"x must be a list but instead is {type(x)}\"\n",
        "  assert isinstance(y, int), f\"y must be an integer but instead is {type(y)}\"\n",
        "\n",
        "  result = []\n",
        "  for i in range(len(x)): \n",
        "    c1 = x[i]\n",
        "    result.append(c1/y)\n",
        "  return result \n",
        "\n",
        "def meanv(matrix:list) -> list:\n",
        "  assert isinstance(matrix, list), f\"matrix must be a list but instead is {type(x)}\"\n",
        "  assert len(matrix) >=1, f\"matrix must have at least one row\"\n",
        "\n",
        "  sumv = matrix[0] # start with the first row\n",
        "  for row in matrix[1:]: # add each row to the first row, starting with the second row\n",
        "    sumv = addv(sumv, row) # take the sum of the first+second row, and then this resulting sum plus the third row\n",
        "  mean = dividev(sumv, len(matrix)) # divide the sum of all the rows by the number of rows\n",
        "  return mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHXpx5HKL9iT",
        "colab_type": "code",
        "outputId": "f8a3f265-9d68-48c3-f450-736ebc3cdef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Get the average vector for the first (0th) sentence in pilot_sentences. Store it in s0_average. Should be of length 300.\n",
        "\n",
        "first_sent = pilot_sentences[0]\n",
        "doc = nlp(first_sent.lower())\n",
        "\n",
        "vectors = []\n",
        "\n",
        "for token in doc:\n",
        "  word = token.text\n",
        "  vecs = get_vec(word)\n",
        "  vectors.append(vecs)\n",
        "\n",
        "s0_average = meanv(vectors)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46f4evmIOgVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(s0_average) # 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx1p1zbSOhN4",
        "colab_type": "code",
        "outputId": "93df1c07-299a-4d31-e29d-f22c90f2ff2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(s0_average[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.02954764893123259, 0.32462666432062787, -0.013579833010832468, -0.2010739967226982, -0.03903430079420408, 0.014894500374794006, -0.01435499886671702, -0.27804166823625565, -0.10192649935682614, 2.4969166914621987]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}